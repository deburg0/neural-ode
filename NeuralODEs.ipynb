{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmK05ZSi8TYy"
      },
      "source": [
        "# Neural Ordinary Differential Equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxvjWyfr8TYz"
      },
      "source": [
        "A significant portion of processes can be described by differential equations: let it be evolution of physical systems, medical conditions of a patient, fundamental properties of markets, etc. Such data is sequential and continuous in its nature, meaning that observations are merely realizations of some continuously changing state.\n",
        "\n",
        "There is also another type of sequential data that is discrete â€“ NLP data, for example: its state changes discretely, from one symbol to another, from one word to another.\n",
        "\n",
        "Today both these types are normally processed using recurrent neural networks. They are, however, essentially different in their nature, and it seems that they should be treated differently.\n",
        "\n",
        "At the last NIPS conference a very interesting [paper](https://arxiv.org/abs/1806.07366) was presented that attempts to tackle this problem. Authors propose a very promising approach, which they call **Neural Ordinary Differential Equations**.\n",
        "\n",
        "Here I tried to reproduce and summarize the results of original paper, making it a little easier to familiarize yourself with the idea. As I believe, this new architecture may soon be, among convolutional and recurrent networks, in a toolbox of any data scientist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZpxQ6lt8TYz"
      },
      "source": [
        "Imagine a problem: there is a process following an unknown ODE and some (noisy) observations along its trajectory\n",
        "\n",
        "$$\n",
        "\\frac{dz}{dt} = f(z(t), t) \\tag{1}\n",
        "$$\n",
        "$$\n",
        "\\{(z_0, t_0),(z_1, t_1),...,(z_M, t_M)\\} - \\text{observations}\n",
        "$$\n",
        "\n",
        "Is it possible to find an approximation $\\widehat{f}(z, t, \\theta)$ of dynamics function $f(z, t)$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRADQL4P8TY0"
      },
      "source": [
        "First, consider a somewhat simpler task: there are only 2 observations, at the beginning and at the end of the trajectory, $(z_0, t_0), (z_1, t_1)$. One starts the evolution of the system from $z_0, t_0$ for time $t_1 - t_0$ with some parameterized dynamics function using any ODE initial value solver. After that, one ends up being at some new state $\\hat{z_1}, t_1$, compares it with the observation $z_1$, and tries to minimize the difference by varying the parameters $\\theta$.\n",
        "\n",
        "Or, more formally, consider optimizing the following loss function $L(\\hat{z_1})$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iHuINqJ8TY0"
      },
      "source": [
        "$$\n",
        "L(z(t_1)) = L \\Big( z(t_0) + \\int_{t_0}^{t_1} f(z(t), t, \\theta)dt \\Big) = L \\big( \\text{ODESolve}(z(t_0), f, t_0, t_1, \\theta) \\big) \\tag{2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr437xF98TY1"
      },
      "source": [
        "<img src=https://github.com/deburg0/neural-ode/blob/master/assets/backprop.png?raw=1 width=600></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T_F9_V98TY1"
      },
      "source": [
        "<p style=\"text-align: center\">Figure 1: Continuous backpropagation of the gradient requires solving the augmented ODE backwards in time. <br /> Arrows represent adjusting backpropagated gradients with gradients from observations. <br />\n",
        "Figure from the original paper</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK6IZmMr8TY1"
      },
      "source": [
        "In case you don't want to dig into the maths, the above figure representes what is going on. Black trajectory represents solving the ODE during forward propagation. Red arrows represent solving the adjoint ODE during backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AKRgubQ8TY2"
      },
      "source": [
        "To optimize $L$ one needs to compute the gradients wrt. its parameters: $z(t_0), t_0, t_1, \\theta$. To do this let us first determine how loss depends on the state at every moment of time $(z(t))$:\n",
        "$$\n",
        "a(t) = \\frac{\\partial L}{\\partial z(t)} \\tag{3}\n",
        "$$\n",
        "$a(t)$ is called *adjoint*, its dynamics is given by another ODE, which can be thought of as an instantaneous analog of the chain rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctc1LffM8TY2"
      },
      "source": [
        "$$\n",
        "\\frac{d a(t)}{d t} = -a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial z} \\tag{4}\n",
        "$$\n",
        "Actual derivation of this particular formula can be found in the appendix of the original paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO_IpZ058TY2"
      },
      "source": [
        "All vectors here are considered row vectors, whereas the original paper uses both column and row representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNsXSLHf8TY2"
      },
      "source": [
        "One can then compute\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z(t_0)} = \\int_{t_1}^{t_0} a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial z} dt \\tag{5}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Daw8ZQxO8TY2"
      },
      "source": [
        "To compute the gradients wrt. to $t$ and $\\theta$ one can think of them as if they were part of the augmented state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NjsY0Ug8TY3"
      },
      "source": [
        "$$\n",
        "\\frac{d}{dt} \\begin{bmatrix} z \\\\ \\theta \\\\ t \\end{bmatrix} (t) = f_{\\text{aug}}([z, \\theta, t]) := \\begin{bmatrix} f([z, \\theta, t ]) \\\\ 0 \\\\ 1 \\end{bmatrix} \\tag{6}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-blLQbd8TY3"
      },
      "source": [
        "Adjoint state to this augmented state is then\n",
        "$$\n",
        "a_{\\text{aug}} := \\begin{bmatrix} a \\\\ a_{\\theta} \\\\ a_t \\end{bmatrix}, a_{\\theta}(t) := \\frac{\\partial L}{\\partial \\theta(t)}, a_t(t) := \\frac{\\partial L}{\\partial t(t)} \\tag{7}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et2lKTKT8TY3"
      },
      "source": [
        "Gradient of the augmented dynamics\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f_{\\text{aug}}}{\\partial [z, \\theta, t]} = \\begin{bmatrix}\n",
        "\\frac{\\partial f}{\\partial z} & \\frac{\\partial f}{\\partial \\theta} & \\frac{\\partial f}{\\partial t} \\\\\n",
        "0 & 0 & 0 \\\\\n",
        "0 & 0 & 0\n",
        "\\end{bmatrix} \\tag{8}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5K8aSeD8TY3"
      },
      "source": [
        "Adjoint state ODE from formula (4) is then\n",
        "$$\n",
        "\\frac{d a_{\\text{aug}}}{dt} = - \\begin{bmatrix} a\\frac{\\partial f}{\\partial z} & a\\frac{\\partial f}{\\partial \\theta} & a\\frac{\\partial f}{\\partial t}\\end{bmatrix} \\tag{9}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J3gf7oK8TY3"
      },
      "source": [
        "By solving this adjoint augmented ODE initial value problem one gets\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z(t_0)} = \\int_{t_1}^{t_0} a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial z} dt \\tag{10}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\theta} = \\int_{t_1}^{t_0} a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta} dt \\tag{11}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial t_0} = \\int_{t_1}^{t_0} a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial t} dt \\tag{12}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O8DtolQ8TY3"
      },
      "source": [
        "which, together with,\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial t_1} = - a(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial t} \\tag{13}\n",
        "$$\n",
        "complements gradients wrt. all the ODESolve parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3yKmu5q8TY3"
      },
      "source": [
        "The gradients (10), (11), (12), (13) can be calculated altogether during a single call of the ODESolve with augmented state dynamics (9)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP_7DB4t8TY3"
      },
      "source": [
        "<img src=https://github.com/deburg0/neural-ode/blob/master/assets/pseudocode.png?raw=1 width=800></img>\n",
        "<div align=\"center\">Figure from the original paper</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-aIo4yM8TY4"
      },
      "source": [
        "The algorithm above describes backpropagation of gradients for the ODE initial value problem with subsequent observations. This algorithm lies in the heart of Neural ODEs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_u5tL4M8TY4"
      },
      "source": [
        "In case there are many observations along the trajectory, one computes the adjoint augmented ODE dynamics for subsequent observations, adjusting the backpropagated gradients with direct gradients at observation times, as shown above on *figure 1*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udUOdYqf8TY4"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itfw-dXU8TY4"
      },
      "source": [
        "The code below is my own implementation of the **Neural ODE**. I did it solely for better understanding of what's going on. However it is very close to what is actually implemented in authors' [repository](https://github.com/rtqichen/torchdiffeq). This notebook collects all the code that's necessary for understanding in one place and is slightly more commented. For actual usage and experiments I suggest using authors'  original implementation.\n",
        "\n",
        "Below is the code if you are interested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PE6Uyogs8TY4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.color_palette(\"bright\")\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.nn  import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "use_cuda = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byg3XoDg8TY4"
      },
      "source": [
        "Implement any ordinary differential equation initial value solver. For the sake of simplicity it'll be Euler's ODE initial value solver, however any explicit or implicit method will do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nLyDS1I28TY5"
      },
      "outputs": [],
      "source": [
        "def ode_solve(z0, t0, t1, f):\n",
        "    \"\"\"\n",
        "    Simplest Euler ODE initial value solver\n",
        "    \"\"\"\n",
        "    h_max = 0.05\n",
        "    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n",
        "\n",
        "    h = (t1 - t0)/n_steps\n",
        "    t = t0\n",
        "    z = z0\n",
        "\n",
        "    for i_step in range(n_steps):\n",
        "        z = z + h * f(z, t)\n",
        "        t = t + h\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzcfXyuD8TY5"
      },
      "source": [
        "We also implement a superclass of parameterized dynamics function in the form of neural network with a couple useful methods.\n",
        "\n",
        "First, one needs to be able to flatten all the parameters that the function depends on.\n",
        "\n",
        "Second, one needs to implement a method that computes the augmented dynamics. This augmented dynamics depends on the gradient of the function wrt. its inputs and parameters.  In order to not have to specify them by hand for every new architecture, we will use **torch.autograd.grad** method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PT3ueRwv8TY5"
      },
      "outputs": [],
      "source": [
        "class ODEF(nn.Module):\n",
        "    def forward_with_grad(self, z, t, grad_outputs):\n",
        "        \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\"\n",
        "        batch_size = z.shape[0]\n",
        "\n",
        "        out = self.forward(z, t)\n",
        "\n",
        "        a = grad_outputs\n",
        "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
        "            (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a),\n",
        "            allow_unused=True, retain_graph=True\n",
        "        )\n",
        "        # grad method automatically sums gradients for batch items, we have to expand them back\n",
        "        if adfdp is not None:\n",
        "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n",
        "            adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
        "        if adfdt is not None:\n",
        "            adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
        "        return out, adfdz, adfdt, adfdp\n",
        "\n",
        "    def flatten_parameters(self):\n",
        "        p_shapes = []\n",
        "        flat_parameters = []\n",
        "        for p in self.parameters():\n",
        "            p_shapes.append(p.size())\n",
        "            flat_parameters.append(p.flatten())\n",
        "        return torch.cat(flat_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSTEAWH88TY5"
      },
      "source": [
        "The code below incapsulates forward and backward passes of *Neural ODE*. We have to separate it from main ***torch.nn.Module*** because custom backward function can't be implemented inside Module, but can be implemented inside ***torch.autograd.Function***. So this is just a little workaround.\n",
        "\n",
        "This function underlies the whole Neural ODE method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YSRjldHp8TY5"
      },
      "outputs": [],
      "source": [
        "class ODEAdjoint(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, z0, t, flat_parameters, func):\n",
        "        assert isinstance(func, ODEF)\n",
        "        bs, *z_shape = z0.size()\n",
        "        time_len = t.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            z = torch.zeros(time_len, bs, *z_shape).to(z0)\n",
        "            z[0] = z0\n",
        "            for i_t in range(time_len - 1):\n",
        "                z0 = ode_solve(z0, t[i_t], t[i_t+1], func)\n",
        "                z[i_t+1] = z0\n",
        "\n",
        "        ctx.func = func\n",
        "        ctx.save_for_backward(t, z.clone(), flat_parameters)\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dLdz):\n",
        "        \"\"\"\n",
        "        dLdz shape: time_len, batch_size, *z_shape\n",
        "        \"\"\"\n",
        "        func = ctx.func\n",
        "        t, z, flat_parameters = ctx.saved_tensors\n",
        "        time_len, bs, *z_shape = z.size()\n",
        "        n_dim = np.prod(z_shape)\n",
        "        n_params = flat_parameters.size(0)\n",
        "\n",
        "        # Dynamics of augmented system to be calculated backwards in time\n",
        "        def augmented_dynamics(aug_z_i, t_i):\n",
        "            \"\"\"\n",
        "            tensors here are temporal slices\n",
        "            t_i - is tensor with size: bs, 1\n",
        "            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n",
        "            \"\"\"\n",
        "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  # ignore parameters and time\n",
        "\n",
        "            # Unflatten z and a\n",
        "            z_i = z_i.view(bs, *z_shape)\n",
        "            a = a.view(bs, *z_shape)\n",
        "            with torch.set_grad_enabled(True):\n",
        "                t_i = t_i.detach().requires_grad_(True)\n",
        "                z_i = z_i.detach().requires_grad_(True)\n",
        "                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a)  # bs, *z_shape\n",
        "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
        "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
        "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n",
        "\n",
        "            # Flatten f and adfdz\n",
        "            func_eval = func_eval.view(bs, n_dim)\n",
        "            adfdz = adfdz.view(bs, n_dim)\n",
        "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
        "\n",
        "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
        "        with torch.no_grad():\n",
        "            ## Create placeholders for output gradients\n",
        "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
        "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
        "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
        "            # In contrast to z and p we need to return gradients for all times\n",
        "            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
        "\n",
        "            for i_t in range(time_len-1, 0, -1):\n",
        "                z_i = z[i_t]\n",
        "                t_i = t[i_t]\n",
        "                f_i = func(z_i, t_i).view(bs, n_dim)\n",
        "\n",
        "                # Compute direct gradients\n",
        "                dLdz_i = dLdz[i_t]\n",
        "                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
        "\n",
        "                # Adjusting adjoints with direct gradients\n",
        "                adj_z += dLdz_i\n",
        "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
        "\n",
        "                # Pack augmented variable\n",
        "                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1)\n",
        "\n",
        "                # Solve augmented system backwards\n",
        "                aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
        "\n",
        "                # Unpack solved backwards augmented system\n",
        "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
        "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
        "                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n",
        "\n",
        "                del aug_z, aug_ans\n",
        "\n",
        "            ## Adjust 0 time adjoint with direct gradients\n",
        "            # Compute direct gradients\n",
        "            dLdz_0 = dLdz[0]\n",
        "            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
        "\n",
        "            # Adjust adjoints\n",
        "            adj_z += dLdz_0\n",
        "            adj_t[0] = adj_t[0] - dLdt_0\n",
        "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkLupLdI8TY6"
      },
      "source": [
        "Wrap ode adjoint function in **nn.Module** for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RtjDY7NC8TY6"
      },
      "outputs": [],
      "source": [
        "class NeuralODE(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super(NeuralODE, self).__init__()\n",
        "        assert isinstance(func, ODEF)\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n",
        "        t = t.to(z0)\n",
        "        z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func)\n",
        "        if return_whole_sequence:\n",
        "            return z\n",
        "        else:\n",
        "            return z[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyTIwVFN8TY6"
      },
      "source": [
        "# Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPrpD3vG8TY6"
      },
      "source": [
        "## _Learning true dynamics function (proof of concept)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZdjpgX-8TY6"
      },
      "source": [
        "As a proof-of-concept we will now test if Neural ODE can indeed restore true dynamics function using sampled data.\n",
        "\n",
        "To test this we will specify an ODE, evolve it and sample points on its trajectory, and then restore it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQflRpAF8TY6"
      },
      "source": [
        "First, we'll test a simple linear ODE. Dynamics is given with a matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp0k2hXv8TY6"
      },
      "source": [
        "$$\n",
        "\\frac{dz}{dt} = \\begin{bmatrix}-0.1 & -1.0\\\\1.0 & -0.1\\end{bmatrix} z\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMx-PO9d8TY_"
      },
      "source": [
        "Trained function here is also a simple matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0guJ48898TY_"
      },
      "source": [
        "The trained function here is also a simple matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhihIDsz8TY_"
      },
      "source": [
        "![leaning gif](https://github.com/deburg0/neural-ode/blob/master/assets/linear_learning.gif?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1EO2BF8TY_"
      },
      "source": [
        "Next, slighty more sophisticated dynamics (no gif as its learning process is not so satisfying :)).  \n",
        "Trained function here is MLP with one hidden layer.\n",
        "![complicated result](https://github.com/deburg0/neural-ode/blob/master/assets/comp_result.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WYnumbhW8TZA"
      },
      "outputs": [],
      "source": [
        "class LinearODEF(ODEF):\n",
        "    def __init__(self, W):\n",
        "        super(LinearODEF, self).__init__()\n",
        "        self.lin = nn.Linear(2, 2, bias=False)\n",
        "        self.lin.weight = nn.Parameter(W)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.lin(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1iavynA8TZA"
      },
      "source": [
        "Dynamics is simply given with a matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z3jIXJ5u8TZA"
      },
      "outputs": [],
      "source": [
        "class SpiralFunctionExample(LinearODEF):\n",
        "    def __init__(self):\n",
        "        super(SpiralFunctionExample, self).__init__(Tensor([[-0.1, -1.], [1., -0.1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSerp_xH8TZA"
      },
      "source": [
        "Initial random linear dynamics function to be optimized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Rmc35unT8TZA"
      },
      "outputs": [],
      "source": [
        "class RandomLinearODEF(LinearODEF):\n",
        "    def __init__(self):\n",
        "        super(RandomLinearODEF, self).__init__(torch.randn(2, 2)/2.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1sE7vMP8TZB"
      },
      "source": [
        "More sophisticated dynamics for creating trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gEAe7a1m8TZB"
      },
      "outputs": [],
      "source": [
        "class TestODEF(ODEF):\n",
        "    def __init__(self, A, B, x0):\n",
        "        super(TestODEF, self).__init__()\n",
        "        self.A = nn.Linear(2, 2, bias=False)\n",
        "        self.A.weight = nn.Parameter(A)\n",
        "        self.B = nn.Linear(2, 2, bias=False)\n",
        "        self.B.weight = nn.Parameter(B)\n",
        "        self.x0 = nn.Parameter(x0)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        xTx0 = torch.sum(x*self.x0, dim=1)\n",
        "        dxdt = torch.sigmoid(xTx0) * self.A(x - self.x0) + torch.sigmoid(-xTx0) * self.B(x + self.x0)\n",
        "        return dxdt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjtxXqA48TZB"
      },
      "source": [
        "Dynamics function to be optimized is MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ptUjAIyz8TZB"
      },
      "outputs": [],
      "source": [
        "class NNODEF(ODEF):\n",
        "    def __init__(self, in_dim, hid_dim, time_invariant=False):\n",
        "        super(NNODEF, self).__init__()\n",
        "        self.time_invariant = time_invariant\n",
        "\n",
        "        if time_invariant:\n",
        "            self.lin1 = nn.Linear(in_dim, hid_dim)\n",
        "        else:\n",
        "            self.lin1 = nn.Linear(in_dim+1, hid_dim)\n",
        "        self.lin2 = nn.Linear(hid_dim, hid_dim)\n",
        "        self.lin3 = nn.Linear(hid_dim, in_dim)\n",
        "        self.elu = nn.ELU(inplace=True)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        if not self.time_invariant:\n",
        "            x = torch.cat((x, t), dim=-1)\n",
        "\n",
        "        h = self.elu(self.lin1(x))\n",
        "        h = self.elu(self.lin2(h))\n",
        "        out = self.lin3(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7d9_6_7_8TZB"
      },
      "outputs": [],
      "source": [
        "def to_np(x):\n",
        "    return x.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WP7O2Ium8TZB"
      },
      "outputs": [],
      "source": [
        "def plot_trajectories(obs=None, times=None, trajs=None, save=None, figsize=(16, 8)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    if obs is not None:\n",
        "        if times is None:\n",
        "            times = [None] * len(obs)\n",
        "        for o, t in zip(obs, times):\n",
        "            o, t = to_np(o), to_np(t)\n",
        "            for b_i in range(o.shape[1]):\n",
        "                plt.scatter(o[:, b_i, 0], o[:, b_i, 1], c=t[:, b_i, 0], cmap=cm.plasma)\n",
        "\n",
        "    if trajs is not None:\n",
        "        for z in trajs:\n",
        "            z = to_np(z)\n",
        "            plt.plot(z[:, 0, 0], z[:, 0, 1], lw=1.5)\n",
        "        if save is not None:\n",
        "            plt.savefig(save)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "x9M7Dzt18TZC"
      },
      "outputs": [],
      "source": [
        "def conduct_experiment(ode_true, ode_trained, n_steps, name, plot_freq=10):\n",
        "    # Create data\n",
        "    z0 = Variable(torch.Tensor([[0.6, 0.3]]))\n",
        "\n",
        "    t_max = 6.29*5\n",
        "    n_points = 200\n",
        "\n",
        "    index_np = np.arange(0, n_points, 1, dtype=int)\n",
        "    index_np = np.hstack([index_np[:, None]])\n",
        "    times_np = np.linspace(0, t_max, num=n_points)\n",
        "    times_np = np.hstack([times_np[:, None]])\n",
        "\n",
        "    times = torch.from_numpy(times_np[:, :, None]).to(z0)\n",
        "    obs = ode_true(z0, times, return_whole_sequence=True).detach()\n",
        "    obs = obs + torch.randn_like(obs) * 0.01\n",
        "\n",
        "    # Get trajectory of random timespan\n",
        "    min_delta_time = 1.0\n",
        "    max_delta_time = 5.0\n",
        "    max_points_num = 32\n",
        "    def create_batch():\n",
        "        t0 = np.random.uniform(0, t_max - max_delta_time)\n",
        "        t1 = t0 + np.random.uniform(min_delta_time, max_delta_time)\n",
        "\n",
        "        idx = sorted(np.random.permutation(index_np[(times_np > t0) & (times_np < t1)])[:max_points_num])\n",
        "\n",
        "        obs_ = obs[idx]\n",
        "        ts_ = times[idx]\n",
        "        return obs_, ts_\n",
        "\n",
        "    # Train Neural ODE\n",
        "    optimizer = torch.optim.Adam(ode_trained.parameters(), lr=0.01)\n",
        "    for i in range(n_steps):\n",
        "        obs_, ts_ = create_batch()\n",
        "\n",
        "        z_ = ode_trained(obs_[0], ts_, return_whole_sequence=True)\n",
        "        loss = F.mse_loss(z_, obs_.detach())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % plot_freq == 0:\n",
        "            z_p = ode_trained(z0, times, return_whole_sequence=True)\n",
        "\n",
        "            plot_trajectories(obs=[obs], times=[times], trajs=[z_p], save=f\"assets/imgs/{name}/{i}.png\")\n",
        "            clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "X5OhbOrR8TZC"
      },
      "outputs": [],
      "source": [
        "ode_true = NeuralODE(SpiralFunctionExample())\n",
        "ode_trained = NeuralODE(RandomLinearODEF())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5OAB5DV78TZC",
        "outputId": "faf0a26e-925c-42c7-9837-0beabda6432e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'assets/imgs/linear/0.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-94443692.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconduct_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mode_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mode_trained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3739125486.py\u001b[0m in \u001b[0;36mconduct_experiment\u001b[0;34m(ode_true, ode_trained, n_steps, name, plot_freq)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mz_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mode_trained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_whole_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mplot_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"assets/imgs/{name}/{i}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2658792614.py\u001b[0m in \u001b[0;36mplot_trajectories\u001b[0;34m(obs, times, trajs, save, figsize)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;31m# savefig default implementation has no return, so mypy is unhappy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;31m# presumably this is here because subclasses can return?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[func-returns-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Need this if 'transparent=True', to reset colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m                     \u001b[0m_recursively_make_axes_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2185\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2039\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[1;32m    429\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2581\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assets/imgs/linear/0.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x800 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABRcAAAK1CAYAAABSNtkuAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAd+9JREFUeJzs3Xe8nQVh//HvuTvr3uy9BwTIAAIEEBUEBQeioFaRKmi1+qutdbSOOuoqdbW2jqKtAg60Ki4cKDIVEDCMQEgC2fNm596su8/vj+DVlCEJuTl3vN+v133d3HOek/uNrOTj85ynUCwWiwEAAAAAOEhlpR4AAAAAAPRM4iIAAAAAcEjERQAAAADgkIiLAAAAAMAhERcBAAAAgEMiLgIAAAAAh0RcBAAAAAAOibgIAAAAABwScREAAAAAOCTiIgAAAABwSHp1XLztttty/vnnZ+zYsSkUCvnRj3500D/Hd7/73Rx//PHp379/Jk2alE9/+tOHfygAAAAA9EC9Oi7u2bMnc+fOzRe/+MVDev0vfvGLvPa1r81b3vKWPPTQQ/nSl76Uf//3f88XvvCFw7wUAAAAAHqeQrFYLJZ6xJFQKBTywx/+MC972cs6H2tubs4//dM/5dvf/nZ27tyZWbNm5ZOf/GTOPPPMJMnFF1+c1tbWfO973+t8zec///l86lOfypo1a1IoFI7wrwIAAAAAuo9efebin/O2t70td955Z77zne9k4cKFeeUrX5nzzjsvjz76aJL98bGmpuaA1/Tr1y/r1q3L6tWrSzEZAAAAALqNPhsX16xZkyuvvDLf+9738uxnPzvTpk3Lu9/97pxxxhm58sorkyTnnntufvCDH+TGG29MR0dHHnnkkXz2s59NkmzcuLGU8wEAAACg5CpKPaBUHnzwwbS3t+eoo4464PHm5uYMGzYsSfKmN70py5cvz0te8pK0tramtrY2b3/72/PP//zPKSvrs10WAAAAAJL04bi4e/fulJeXZ8GCBSkvLz/guYEDBybZ/z6Nn/zkJ/Mv//Ivqa+vz4gRI3LjjTcmSaZOnXrENwMAAABAd9Jn4+IJJ5yQ9vb2bN68Oc9+9rOf8tjy8vKMGzcuSfLtb387p512WkaMGHEkZgIAAABAt9Wr4+Lu3buzbNmyzq9XrlyZ+++/P0OHDs1RRx2V1772tXnd616Xz372sznhhBOyZcuW3HjjjZkzZ05e/OIXZ+vWrfn+97+fM888M01NTZ3v0XjrrbeW8FcFAAAAAN1DoVgsFks9oqvccsstOeussx73+Otf//pcddVVaW1tzcc//vF8/etfz/r16zN8+PCceuqp+chHPpLZs2dn69atOf/88/Pggw+mWCzmtNNOyyc+8YnMnz+/BL8aAAAAAOheenVcBAAAAAC6jlseAwAAAACHRFwEAAAAAA5Jr7uhS0dHRzZs2JBBgwalUCiUeg4AAAAA9CjFYjG7du3K2LFjU1b21Ocm9rq4uGHDhkyYMKHUMwAAAACgR1u7dm3Gjx//lMf0urg4aNCgJPt/8bW1tSVeAwAAAAA9S2NjYyZMmNDZ2Z5Kr4uLf7gUura2VlwEAAAAgEP0dN5y0A1dAAAAAIBDIi4CAAAAAIdEXAQAAAAADom4CAAAAAAcEnERAAAAADgk4iIAAAAAcEjERQAAAADgkIiLAAAAAMAhERcBAAAAgEMiLgIAAAAAh0RcBAAAAAAOibgIAAAAABwScREAAAAAOCTiIgAAAABwSMRFAAAAAOCQiIsAAAAAwCERFwEAAACAQyIuAgAAAACHRFwEAAAAAA6JuAgAAAAAHBJxEQAAAAA4JOIiAAAAAHBIxEUAAAAAeJrW/3Z6Gh6dnqvfcmqpp3QLRyQufvGLX8zkyZNTU1OT+fPn5+67737K47/3ve9l5syZqampyezZs/Pzn//8SMwEAAAAgCfU0DA9Dc1TM/DkjmRSR172H5vTsG9a1v92eqmnlVRFV3+D//3f/8073/nOXHHFFZk/f34+97nP5dxzz83SpUszcuTIxx1/xx135DWveU0uv/zyvOQlL8k111yTl73sZbn33nsza9asrp4LAAAAQBfr6Chmd0tbdjW1pXFf6x8/N7dmX0tH9rW2p6m1Pfta2rO3pf2Ar/e17v9obm1Pa3sx7R3FtHZ0pL2jmLb2Yto6Oh77XEyhkBSSlBUK+39cKKSQpLK8LNUVZamq2P+5uqI8A6rLU9evcv9H/6rU9avM8IFVGT+kXwZVnZMRg4v7z9IrPPaLKCQpK2bgycWs/+30jDtjWan+5yypQrFYLHblN5g/f35OPvnkfOELX0iSdHR0ZMKECfnbv/3bvPe9733c8X/xF3+RPXv25Kc//WnnY6eeemqOP/74XHHFFX/2+zU2Nqauri4NDQ2pra09fL8QAAAAAA7Q0taRHXtbsm13S7bvacm2Pc3ZvqclO/a2pnFfaxqbWtO4ry27mlrT2BkSW7OruS1dW6QOv+qKlhw1en3mjF+Vs45ZmJMmL0uhkKSYpKOQun7LSz3xsDmYvtalZy62tLRkwYIFed/73tf5WFlZWc4555zceeedT/iaO++8M+985zsPeOzcc8/Nj370o66cCgAAANDnNbW2Z9uelmzf/cdQuD8a/uGxlmx/7PFte1qyq6ntGX2/qvKy1ParSG1NZQbVVGRgTUX6V1WkX2X5/o+q8tRUlqd/1f6vax773L+qPNUVZakoL0tFWWH/R3khFWVlnZ/Ly/afYlgsFvf3v2IxxeL+z23txbS0d6S5tSPNbe1pau3Inua27NzXkoZ9rdm5tzU797Vmy67mrNv+aDbtGpzmtqo8uG5KHlw3Jd/63Vm5cN4d+fAF3+48g/Hqt5ya11/xu2f+F6GH6dK4uHXr1rS3t2fUqFEHPD5q1KgsWbLkCV9TX1//hMfX19c/4fHNzc1pbm7u/LqxsfEZrgYAAADoPYrFYhqb2rJlV1M2NzZn867mbNnVnM27mrJ5V/NjjzVly67mNB5CLCwrJEMHVHV+DBtQnSED9l9ePKimMrU1lantV/HYjx/7/FhQrKks74Jf8eHV0HRJ2joKWbdzeBZvmJD3fv/SJMkPFpy+Py4mSSF52bu2lm5kCXX5ey52tcsvvzwf+chHSj0DAAAA4Ihr7yhm2+7mbGxoSn1jU+r/9POf/Hhfa/vT/jkrywuPhcLqDDsgGlZl6MDHPg+o7nysrl9lysoKf/4n7sEqyjsyefjmTB6+OcUkn/vVBXntabcccMzuTUndjJLMK6kujYvDhw9PeXl5Nm3adMDjmzZtyujRo5/wNaNHjz6o49/3vvcdcBl1Y2NjJkyY8AyXAwAAAJRWsVhM4762rN+5L+t37suGxz7+8OP6hqZs2tWc9o6n9+aFg2oqMnJQdUYOqsnI2uqMHFSdEX/4elB1RtZWZ8TAmtT2q0ih0Ltj4UFpKSQ1f/zf+EVzFuRFcxb88fnHnuqrN3Tp0rhYVVWVefPm5cYbb8zLXvayJPtv6HLjjTfmbW972xO+5rTTTsuNN96Yv//7v+987IYbbshpp532hMdXV1enurr6cE8HAAAA6FLtHcVsamzKuh37sn7n3mzY2bQ/JO74Y0jc0/LnzzgsKyQjB9VkVF1NxtTWZHTdYx+1f/w8qrYm/aq6/yXI3dEtnxyeMz+0ef8X/7e5/qE5NpclfTRPdfll0e985zvz+te/PieddFJOOeWUfO5zn8uePXty2WWXJUle97rXZdy4cbn88suTJG9/+9vz3Oc+N5/97Gfz4he/ON/5znfy+9//Pl/5yle6eioAAADAYVMsFrNtT0vWbt+btTv2Ze32vVm3Y2/Wbt+XdTv2Zv3OfWlt//NnHQ4bUJVxQ/plbF2/jB3cL2MH12Ts4H4ZU1eTMXX9MnxgVSrKy47Ar6hvuuATv0vDqunJmI79D/zfv2RthdTV9s2zFpMjEBf/4i/+Ilu2bMmHPvSh1NfX5/jjj8/111/fedOWNWvWpKzsj/8AnH766bnmmmvygQ98IO9///szY8aM/OhHP8qsWbO6eioAAADAQdnX0p412/dm9bY9WbN97/8Jifv+7HsdVpQVMnZwv4wf8odw2C/jBx8YEXvCTU96u7rJy/Ljfzo1Z75na1L1WF0sJtlQSN20vhsWk6RQLBaf3oX5PURjY2Pq6urS0NCQ2traUs8BAAAAerhdTa1ZvW1vVm/bm1Xb9mT1tj2dX9c3Nj3lawuFZHRtTcYP6ZcJQ/pn/ND+mTCkXyYM7Z8JQ/tndG1Nynv5zVDoeQ6mr/X4u0UDAAAAPFM797ZkxdY9WdMZEPd2RsRte1qe8rWDaioyediATBzWPxOG9M+Eof0e+9w/YwfXpLrCmYf0XuIiAAAA0Cc0tbZn9ba9Wbl1d5Zv2ZOVW/dkxZbdWbl1T3bsbX3K1w4fWJVJwwZk0tD+mTRsQCYP75+JQ/tn8rABGdy/0t2V6bPERQAAAKDX6OgoZmNjU2c0XLFlT1Y8FhHX79yXp3pzuDF1NZk0bH8wnDRsQCYN6//Yx4AMrJZQ4In4JwMAAADocfa1tGf5lt1Ztnn/x4qtu7Niy56s2rYnTa0dT/q6QTUVmTpiYKYOH5CpwwdkyogBmTp8YKYMH5B+VS5fhoMlLgIAAADdVsPe1izbsqszIj762Od1O/Y96WsqywuZOLT/HyPiiAGZMnxgpo4YkGEDqlzCDIeRuAgAAACUVLFYzJbdzVm2eXeW/0lAfHTz7mzZ1fykrxs6oCrTRwzMtJEDM23E/og4dfjAjB/SLxXlZUfwVwB9l7gIAAAAHBHFYjGbGpuzdNOuPLpp1wFnIjbse/Ibqoypq8n0kQMzbcTAzBg1MNNHDMz0kQMzbGD1EVwPPBFxEQAAADjsGva2ZummXfs/6hvzSP3uLN2060kjYqGQTBzaPzNG7j8TcfqIgZkxalCmjRiQQTWVR3g98HSJiwAAAMAha2ptz7LNu7O0/g8hcf9HfWPTEx5fXlbIlOEDctQfzkAcNSjTR+x/P8SaSjdUgZ5GXAQAAAD+rPaOYlZt25NH6ndlSf2uPPJYSFy1bU86ik/8mnGD++WoUQNz9OjaHD16YI4eVSsiQi8jLgIAAAAH2L6nJYs3NmbxxsY8vLExS+v3vz9ic1vHEx4/uH9ljh41KDNHD8pRo/d/njFqUGpdzgy9nrgIAAAAfVRHRzGrt+/Nwxv+GBIXb2zMxoYnvqS5prIsR40alKNHDcrRox/7GDUoIwZVp1AoHOH1QHcgLgIAAEAfsK+lPUvqG7N44648vLEhD29ozJL6Xdnb0v6Ex08a1j/HjK7NMWNqc/RjZyNOGNo/5WUiIvBH4iIAAAD0IsViMVt2Nefhx85E/MNZiSu3PvF7I1ZXlOXo0YNy7JjaHDt2f0ycOXqQOzQDT4u4CAAAAD1Ue0cxK7bs7oyIf7iseevulic8fvjAqhzzWEQ8dsz+jynDB6SivOwILwd6C3ERAAAAeoC29o4s37InD65vyEPrG/Lg+v2XNu9rffxlzWWFZMrwATl2bF2OHVObY8YMyrFjazNyUE0JlgO9mbgIAAAA3Uxbe0ce3bw7D/1pSNzYmKbWx9+tuX9V+f6zEf/ksuajRw1Kv6ryEiwH+hpxEQAAAEqotb0jj27a3RkRH1zfkMUbG9Pc9viQOKCqPMeNrcuscXWZPb42s8fVZcrwgW6yApSMuAgAAABHSGt7Rx7ZtOtPQmJjljxJSBxYXZFjx+4PiLPH7Q+KU4cPSJmQCHQj4iIAAAB0gZa2A0PiQ+sbsrh+V1qeICQOqq7IceNqOyPirHF1mTJMSAS6P3ERAAAAnqFisZjV2/bmgXU7c9+anXlg3c4s2tD4xCGxpiKzxtZl9vjHLm8eV5dJQ/sLiUCPJC4CAADAQdq+pyUPrN2Z+x/7eGDdzuzc2/q442prKjoD4h8+TxQSgV5EXAQAAICn0NTankUbGvdHxMdi4prtex93XFV5WY4dW5vjJwzO8RMGZ+6EwZk8rH8KBSER6L3ERQAAAHhMR0cxK7bu7ry0+f61O7Nk4660dRQfd+zUEQNy/PjBOX7i4MwdPzjHjKlNVUVZCVYDlI64CAAAQJ+1ubHpgEubF65tyK7mtscdN3xg1f6zER+LiXPGDU5d/8oSLAboXsRFAAAA+oQ9zW15cH1D56XND6zdmQ0NTY87rl9leWaPq8vcCXU5fsKQzJ1Ql3GD+7m8GeAJiIsAAAD0OsViMet27Mu9a3Zkwer9H4s3Nub/Xt1cKCRHjRzU+R6Jx08YnKNGDUxFucubAZ4OcREAAIAer6WtI4s2NGTB6h2dQXFTY/PjjhtTV9MZEueOH5zZ4+sysNofjQEOlX+DAgAA0ONs292ce9fsfOysxO1ZuK4hzW0dBxxTUVbIcePqMm/ikMybNCQnThqcMXX9SrQYoHcSFwEAAOjWOjqKWbZld36/akfnmYkrt+553HFD+lc+FhGHZN7EIZkzfnD6VZWXYDFA3yEuAgAA0K3sbm7LA2t3dr5X4r1rdmRX0+Pv4Dxj5MDMmzSk82PK8AFuugJwhImLAAAAlMzTvfFK/6ryHD9hcOeZiSdOGJK6/pWlGQ1AJ3ERAACAI+ZPb7zyh4/Nux5/45Vxg/sdcFbizNGD3MEZoBsSFwEAAOgy+1rac9/aHbl75fbctWJ77lu7I02tT33jlXmThmR0XU2JFgNwMMRFAAAADptdTa35/er9MfHulduzcN3OtLYfeI3zH268Mm/S0MybNCRzxtelptKNVwB6InERAACAQ7Z9T0vuWbX/rMS7V23Lwxse/36Jo2trMn/q0JwyZWjmTxmaaSMGuvEKQC8hLgIAAPC0bWpsyl0rt+fuldty98rteWTT7scdM2lY/5wy+Q8xcVgmDO0nJgL0UuIiAAAAT+gPd3K+a+X23LViW+5etT2rt+193HFHjRqYU6YMzSlThuWUyUO9XyJAHyIuAgAAkGR/TFy+ZfdjZybu/9jY0HTAMWWF5NixtTll8rDHguLQDB1QVaLFAJSauAgAANBHtXcUs6S+sfNOzves2p5te1oOOKayvJA54wd3hsR5k4aktqayRIsB6G7ERQAAgD6itb0jD65v6Dwr8Z5V27Orqe2AY6orynLixCGdN185YeKQ9KtyJ2cAnpi4CAAA0Eu1dxSzeGNj7li+Nbcv25Z7Vm3P3pb2A44ZWF2Rkyb/MSbOHjc4VRVlJVoMQE8jLgIAAPQS+98zcU/uWL41dyzbljtXbEvDvtYDjhnSvzIn/8mdnI8ZMygV5WIiAIdGXAQAAOjB1u3YmzuWb8udy7fljuVbs6mx+YDnB1ZXZP6UoTl9+vCcPm1Yjh41KGVlhRKtBaC3ERcBAAB6kK27mztD4h3Lt2X1tr0HPF9VUZaTJg3Js6YPz2nThmXOuDpnJgLQZcRFAACAbqyxqTV3rdjeeanz0k27Dni+vKyQuePrcvq04Tl9+rCcOHFIairdgAWAI0NcBAAA6EaaWtvz+1U79t+EZfm2PLhuZzqKBx5zzJjaPGvasJw+fVhOnjw0g2oqSzMWgD5PXAQAACih1vaOLFy3M7cv23+p872rd6alveOAY6YMH5DTpw3L6dP2X+o8dEBVidYCwIHERQAAgCOoo6OYhzc2dr5v4t0rt2dPS/sBx4yurcnp04flWY/FxLGD+5VoLQA8NXERAACgCxWLxazYuid3LN+WO5ZtzZ0rtmXn3tYDjhnSvzKnPXZm4unThmXK8AEpFNzRGYDuT1wEAAA4zLbtbs5vl23NbY9sze3Ltqa+semA5wdUlWf+1GGdlzrPHD0oZWViIgA9j7gIAADwDLW2d+S+NTtz6yObc9sjW/PQhoYU/+QmLFUVZZk3ccj+mDh9eOaMr0tleVnpBgPAYSIuAgAAHIK12/fm1ke25LZHtuSO5duyu7ntgOdnjh6U5x41Is+eMSInTR6SmsryEi0FgK4jLgIAADwNe5rb8rsV23LbI1ty26Nbs3LrngOeHzqgKmdMH57nHDUiz5kxPCNra0q0FACOHHERAADgCRSL++/qfNsjW3PbI1vy+9Xb09r+x2udK8oKOXHikDznqP1BcdbYOu+bCECfIy4CAAA8Zuvu5vz20a2dZydu3d18wPMThvbLc2aMyHOOGpHTpw3LoJrKEi0FgO5BXAQAAPqslraO3Ltmx2MxcUseWt94wPP9q8pz2tRh+y91PmpEJg/rn0LB2YkA8AfiIgAA0Kes3rYntz2yJbc+sjV3Lt+aPS3tBzx/7Jjax2Li8MybNCTVFW7EAgBPRlwEAAB6td3Nbblz+bbOsxNXb9t7wPPDBlTl2TP2v2/iGTOGZ+QgN2IBgKdLXAQAAHqVjo79N2K59ZEtue2RLbl3zY7H3Yhl3qQhec5RI/Lco0bk2DG1bsQCAIdIXAQAAHq8Lbua85tH98fE3y7bmq27Ww54ftKw/p03Yjlt2rAMrPZHIQA4HPwXFQAA6HGKxWIWbWjMjYs356Ylm/LAuoYDnh9QVZ7Tpg3Pc4/af7nzpGEDSrQUAHo3cREAAOgR9rW05/ZlW3Pjkv1BcVNj8wHPzxpX23l24okTh6SqoqxESwGg7xAXAQCAbmv9zn25acnm3LR4U+5Yvi3NbR2dz/WvKs+zZwzP2TNH5cyZI9yIBQBKQFwEAAC6jfaOYu5fuzM3LdmUGxdvzpL6XQc8P35Iv5xzzKg8b+bIzJ86NNUV5SVaCgAk4iIAAFBiu5pa85tHt+bXizfllqVbsn3PH2/GUlZI5k0akufNHJWzjxmZGSMHplBwZ2cA6C7ERQAA4IhbtXVP53sn3rVie9o6ip3PDaqpyJlHj8zZM0fmuUeNyJABVSVcCgA8FXERAADocq3tHfn9qh37L3desjkrtuw54PlpIwbk7Mcud543aUgqy92MBQB6AnERAADoEjv2tOTWR7bkxiWbc+vSzWlsaut8rqKskPlTh+6/3HnmyEwePqCESwGAQyUuAgAAh0WxWMyjm3fnxsX7L3desHpH/uRq5wwdUJWzjh6Zs48ZmTNmDE9tTWXpxgIAh4W4CAAAHLLmtvbctWJ7bly8/3LndTv2HfD8zNGD9t/d+ZiRmTt+cMrL3IwFAHoTcREAADgom3c15ZYlW3Ljkk35zaNbs7elvfO5qoqyPGvasDzvsfdPHDe4XwmXAgBdTVwEAACeUrFYzKINjZ2XOz+wruGA50fVVne+d+Lp04elf5U/ZgBAX+G/+gAAwOO0tnfkrhXbc/2ijbnh4U3Z1Nh8wPNzJwzO2TNH5nkzR+a4sbUpFFzuDAB9kbgIAAAkSZpa23PbI1ty/aL63Lh4cxr2tXY+17+qPM+eMTxnzxyVM2eOyMhBNSVcCgB0F+IiAAD0YY1Nrbl5yeZc/1B9blm6Jfta//j+icMHVuX5x47KC44bndOnDUt1RXkJlwIA3ZG4CAAAfczW3c254eFNuf6h+tyxfGta24udz40b3C/nHjc6580anXmThri7MwDwlMRFAADoA9bt2JtfLtqUXz5Un3tWb0/xjz0x00cOzHmPBUXvnwgAHAxxEQAAeqllm3fl+ofqc/2i+jy0vvGA5+aMr8u5x43OuceNzvSRA0u0EADo6cRFAADoJYrFYh5c39AZFFds2dP5XFkhOXny0Jw3a3RecNzojBvcr4RLAYDeQlwEAIAerL2jmHtWbc/1D9XnV4vqs6GhqfO5qvKyPGv6sJw3a3TOOWZUhg2sLuFSAKA3EhcBAKCHaW5rzx3LtuX6h+pzw+JN2b6npfO5/lXlOevokTl31uicdfSIDKqpLOFSAKC3ExcBAKAH2NPclluWbsn1i+pz85LN2d3c1vnc4P6VOeeYUTn3uNF59ozhqaksL+FSAKAvERcBAKCb2rGnJb9evCm/XFSf2x7dmpa2js7nRtVWd96Q5ZQpQ1NZXlbCpQBAXyUuAgBAN1Lf0JRfPVyf6x+qz10rt6e9o9j53KRh/XPecaNz7qzROX784JSVFUq4FABAXAQAgJJbuXVPfrlof1C8f+3OA547Zkxtzj1uVM6bNTpHjxqUQkFQBAC6D3ERAACOsGKxmIc3NuaXizbllw/VZ+mmXQc8P2/SkJx73P73UJw0bECJVgIA/HniIgAAHAHFYjGLN+7KdQs35GcLN2bN9r2dz1WUFXLatGF5wXGj84JjR2VUbU0JlwIAPH3iIgAAdKHlW3bnugc25LoHNmT5lj2dj1dXlOU5R43IeceNztnHjMzg/lUlXAkAcGjERQAAOMzWbt+bny7cmOse2JCHNzZ2Pl5VUZazjh6Rl8wZm7OPGZn+VX47DgD0bH43AwAAh8Gmxqb8bOHGXLdwQ+5bs7Pz8YqyQs6YMTznzxmb5x83KrU1laUbCQBwmHVpXNy+fXv+9m//Ntddd13Kyspy0UUX5T/+4z8ycODAJ33NmWeemVtvvfWAx/76r/86V1xxRVdOBQCAg7Z9T0t+8dD+MxTvWrk9xeL+xwuF5NQpw3L+3LE5b9boDB3gkmcAoHfq0rj42te+Nhs3bswNN9yQ1tbWXHbZZXnzm9+ca6655ilf96Y3vSkf/ehHO7/u379/V84EAICnrWFfa361qD4/Xbgxv122Ne0dxc7n5k0akvPnjMmLZo/JSDdlAQD6gC6Li4sXL87111+fe+65JyeddFKS5POf/3xe9KIX5TOf+UzGjh37pK/t379/Ro8e3VXTAADgoOxtacuvF2/OdQ9syK1Lt6SlvaPzuVnjanP+nLF58ZwxGT/E/ykOAPQtXRYX77zzzgwePLgzLCbJOeeck7Kystx11115+ctf/qSv/da3vpVvfvObGT16dM4///x88IMfdPYiAABHVFNre259ZEuue2BDbly8Ofta2zufmz5yYF46d2xeMmdMpo548rf8AQDo7bosLtbX12fkyJEHfrOKigwdOjT19fVP+rqLL744kyZNytixY7Nw4cK85z3vydKlS/ODH/zgCY9vbm5Oc3Nz59eNjY1PeBwAAPw5re0duX3Z1lz3wMb8alF9djW3dT43cWj/nD93TM6fOzZHjxqUQqFQwqUAAN3DQcfF9773vfnkJz/5lMcsXrz4kAe9+c1v7vzx7NmzM2bMmJx99tlZvnx5pk2b9rjjL7/88nzkIx855O8HAEDf1t5RzN0rt+e6hRvyiwc3Zsfe1s7nxtTV5CVz9gfF2ePqBEUAgP/joOPiu971rlx66aVPeczUqVMzevTobN68+YDH29rasn379oN6P8X58+cnSZYtW/aEcfF973tf3vnOd3Z+3djYmAkTJjztnx8AgL6nWCzmvrU7c90DG/KzhRuzedcfr4QZPrAqL5q9PyjOmzgkZWWCIgDAkznouDhixIiMGDHizx532mmnZefOnVmwYEHmzZuXJLnpppvS0dHRGQyfjvvvvz9JMmbMmCd8vrq6OtXV1U/75wMAoG8qFot5eGNjrntgY657YEPW79zX+VxtTUVeOGt/UDx16tBUlJeVcCkAQM9RKBaLxa76yV/4whdm06ZNueKKK9La2prLLrssJ510Uq655pokyfr163P22Wfn61//ek455ZQsX74811xzTV70ohdl2LBhWbhwYd7xjndk/PjxufXWW5/W92xsbExdXV0aGhpSW1vbVb80AAB6iGWbd+0Pigs3ZMWWPZ2PD6gqz/OPHZXz547Ns2eMSFWFoAgAkBxcX+uyG7ok++/6/La3vS1nn312ysrKctFFF+U///M/O59vbW3N0qVLs3fv3iRJVVVVfv3rX+dzn/tc9uzZkwkTJuSiiy7KBz7wga6cCQBAL7N2+95ct3BDrntgYxZv/OMN/6oryvK8mSNz/tyxOevokelXVV7ClQAAPV+XnrlYCs5cBADom+obmvLThRty3cKNeWDtzs7HK8sLec6MEXnJ3DE555hRGVRTWbqRAAA9QLc5cxEAALrStt3N+flD9bnugQ25Z9X2/OH/Ni8rJKdPG57z547JuceNzuD+VaUdCgDQS4mLAAD0KE2t7bnh4U259t51+c2jW9Pe8ccLcU6ePCTnzx2bF84akxGD3PQPAKCriYsAAHR7xWIx967Zke8vWJ+fLtyQXU1tnc/NGV+X8+eMzYvnjMnYwf1KuBIAoO8RFwEA6LbW7dibH9y7Pj+4d11Wbdvb+fi4wf1y4YnjcuGJ4zNl+IASLgQA6NvERQAAupU9zW35+YMbc+296/K7Fds7H+9fVZ4XzhqTi+aNy6lThqWsrFDClQAAJOIiAADdQEdHMXeu2JZrF6zLLx6qz77W9iRJoZCcNnVYLjpxfM6bNToDqv32FQCgO/G7MwAASmbFlt259t51+eG967Ohoanz8SnDB+SiE8fl5SeOzzjvowgA0G2JiwAAHFENe1tz3cINufbedblvzc7OxwfVVOT8uWNz0Ynjc+LEwSkUXPYMANDdiYsAAHS5tvaO3Pbolly7YH1uWLwpLW0dSZLyskKeM2N4Lpo3PuccMyo1leUlXgoAwMEQFwEA6DKLNzbm2gXr8qP7N2Tr7ubOx2eOHpSLThyfC04Ym5GDakq4EACAZ0JcBADgsNq6uzk/vn9Drl2wLg9vbOx8fNiAqrz0+P2XPR83ttZlzwAAvYC4CADAM9bc1p6bFm/Otfeuyy1Lt6Sto5gkqSwv5OyZo3LRvPE58+gRqSwvK/FSAAAOJ3ERAIBDUiwW88C6hly7YF2uW7ghO/e2dj43d3xdLpo3PufPGZshA6pKuBIAgK4kLgIAcFA2NuzLD+9bn2sXrMvyLXs6Hx9VW52XnzA+F504LjNGDSrhQgAAjhRxEQCAP2tfS3t+uag+1967Lr9dtjXF/Vc9p6ayLOceNzoXnTg+z5o+POVl3kcRAKAvERcBAHhCHR3F3LNqe669d11+/mB9dje3dT53yuShuWjeuLxo9pgMqqks4UoAAEpJXAQA4ABrtu3Ntfeuyw/uW5e12/d1Pj5haL9ceML4XHjiuEwaNqCECwEA6C7ERQAAsqupNT9/cGOuXbA+d6/a3vn4wOqKvGj2/sueT548NGUuewYA4E+IiwAAfVR7RzG3L9uaa+9dl18uqk9Ta0eSpFBIzpg+PBedOD7nHjc6/arKS7wUAIDuSlwEAOhj1u3Ym+/cvTbfX7Au9Y1NnY9PGzEgF80bn5efMC5j6vqVcCEAAD2FuAgA0Ae0tXfkpiWbc83da3LrI1s67/Zc168yL507NhfNG5+54+tSKLjsGQCAp09cBADoxTY27Mt37l6b/71n7QFnKZ4+bVgunj8xzz92VKorXPYMAMChERcBAHqZ9o5ibn1kc665a01uWrI5HY+dpTh0QFVeMW98XnPKxEwZ7m7PAAA8c+IiAEAvsamxKf97z/6zFNfv3Nf5+PwpQ3Px/Ik5b9ZoZykCAHBYiYsAAD1YR0cxtz26JdfctSY3Ltmc9sdOUxzcvzIXnbj/LMXpIweWeCUAAL2VuAgA0ANt3tWU7/1+Xb5995qs2/HHsxRPnjwkF8+fmBfOGpOaSmcpAgDQtcRFAIAeoqOjmDuWb8s1d6/OrxZtSttjZykOqqnIRSeOz8XzJ+aoUYNKvBIAgL5EXAQA6Oa27m7O9xfsP0tx9ba9nY+fMHFwLj5lYl4yZ2z6VTlLEQCAI09cBADohorFYu5csS3X3LUmv1xUn9b2x85SrK7Iy04Yl4vnT8wxY2pLvBIAgL5OXAQA6Ea272nJtY+dpbhi657Ox+eOr8vF8yfm/Llj07/Kb+EAAOge/M4UAKDEisVi7lm1I9+6a3V+8WB9Wto7kiQDqspzwQnjcvEpEzNrXF2JVwIAwOOJiwAAJdKwtzXX3rsu19y9Jss27+58fNa42lx8yqS89PixGVjtt2sAAHRffrcKAHAEFYvF3LtmR75115r8bOHGNLftP0uxX2V5Ljh+bC6ePzFzxg8u7UgAAHiaxEUAgCOgYV9rfnTf+lxz15os3bSr8/FjxtTm4vkT87Ljx2ZQTWUJFwIAwMETFwEAukixWMz9a3fmmrvW5LqFG9LUuv8sxZrKsrxkzv6zFE+YMDiFQqHESwEA4NCIiwAAh9muptb8+P4NueauNXl4Y2Pn40eNGpiLT5mYl584PnX9nKUIAEDPJy4CABwmD65ryDV3r86P79+QvS3tSZKqirK8ZPaYXDx/YuZNGuIsRQAAehVxEQDgGdjT3JafPLD/LMUH1zd0Pj5txIBcPH9SLjpxXAb3ryrhQgAA6DriIgDAIVi0oSHX3LUmP75/Q3Y3tyVJqsrL8sLZo3PxKRNzypShzlIEAKDXExcBAJ6mfS3tue6BDfnW3WvywNqdnY9PHT4grzllYi6aNz5DBzhLEQCAvkNcBAD4M+obmvKN363KNXetyY69rUmSyvJCzj1udC6ePzGnTR3mLEUAAPokcREA4EksXLczX/3tyvxs4ca0dRSTJBOG9str50/KK+aNz/CB1SVeCAAApSUuAgD8ibb2jvzq4U352m9X5verd3Q+fsqUoXnDs6bk+ceOSnmZsxQBACARFwEAkiSNTa3537vX5qo7VmX9zn1J9l/6fP6csXnDGVMya1xdiRcCAED3Iy4CAH3aqq17ctUdq/K936/Nnpb2JMnQAVV57fyJ+ctTJ2VkbU2JFwIAQPclLgIAfU6xWMydK7bla79dlRuXbEpx/9sp5qhRA/PGM6bkguPHpaayvLQjAQCgBxAXAYA+o7mtPT+5f0O+dvuqLN7Y2Pn4WUePyBvPmJpnTXfXZwAAOBjiIgDQ623Z1Zxv3bU63/zd6mzd3ZIk6VdZnlfMG59LnzU500YMLPFCAADomcRFAKDXenhDY668fWV+fP+GtLR3JEnG1NXk9adPzqtPnpDB/atKvBAAAHo2cREA6FU6Ooq5acnmfPW3K3Pnim2djx8/YXDeeMaUnDdrdCrLy0q4EAAAeg9xEQDoFfY0t+X7C9blyttXZtW2vUmS8rJCXjhrdN5wxpScOHFIiRcCAEDvIy4CAD3auh178/U7V+fbd6/Jrqa2JEltTUVeM39iXnfa5Iwb3K/ECwEAoPcSFwGAHqdYLObeNTvy1d+uzPUP1aejuP/xqcMH5LJnTc6FJ47PgGq/zQEAgK7md90AQI/R2t6Rnz+4MV/77co8sK6h8/Ezpg/PG86YnDOPGpmyskIJFwIAQN8iLgIA3d6OPS255u41+cadq1Pf2JQkqaooy8uPH5fLzpicmaNrS7wQAAD6JnERAOi2lm3ela/dvio/uHddmlo7kiTDB1bndadNymvnT8ywgdUlXggAAH2buAgAdCvFYjG3Pbo1X/vtytz6yJbOx48dU5s3njElL5k7JtUV5SVcCAAA/IG4CAB0C02t7fnBvetz5e0r8+jm3UmSQiF5/jGj8oYzpmT+lKEpFLyfIgAAdCfiIgBQUpsam/L1O1flmrvWZMfe1iTJgKryvOrkCbn09MmZNGxAiRcCAABPRlwEAEpi4bqd+dpvV+anCzemraOYJBk/pF8uPX1yXnXyhNTWVJZ4IQAA8OeIiwDAEdPW3pEbHt6Ur/52ZX6/ekfn46dMHpo3nDE5zz92dMrLXPoMAAA9hbgIAHS5xqbWfPeetbny9lVZv3NfkqSirJDz547NG541JbPH15V4IQAAcCjERQCgy6zauidX3bEq3/v92uxpaU+SDOlfmdfOn5S/PG1SRtXWlHghAADwTIiLAMBht2hDQ75487L84qH6FPe/nWJmjByYN5wxJS8/YVxqKstLOxAAADgsxEUA4LBZsHpHvnjzsty0ZHPnY2cePSJveNaUPHvG8BQK3k8RAAB6E3ERAHhGisVi7lyxLV+4aVnuWL4tSVJWSF4yZ2z+31nTMnN0bYkXAgAAXUVcBAAOSbFYzM1LN+cLNy3LvWt2Jtl/k5YLTxyXt545PVOGDyjtQAAAoMuJiwDAQenoKOb6RfX54s3LsmhDY5KkqqIsrz55Qt78nKkZP6R/iRcCAABHirgIADwtbe0duW7hhnzx5uVZtnl3kqR/VXkuOXVS/uqMKRnpzs8AANDniIsAwFNqbmvPtQvW54pbl2fN9r1JkkE1Fbns9Mm57FlTMmRAVYkXAgAApSIuAgBPaF9Le75995p85bYVqW9sSpIMHVCVN54xJX952qTU1lSWeCEAAFBq4iIAcIBdTa35xu9W56u/WZlte1qSJKNqq/Pm50zLa06ZkP5VfvsAAADs508HAECSZOfelnzt9lW56vaVaWxqS5KMH9Ivbz1zWl4xb3yqK8pLvBAAAOhuxEUA6OM272rKV3+zMt/83ersaWlPkkwbMSD/78zpeenxY1NZXlbihQAAQHclLgJAH7V+57585dbl+c49a9Pc1pEkOWZMbd521vScN2t0yssKJV4IAAB0d+IiAPQxq7buyX/dsjw/uG9dWtuLSZLjJwzO3z5vep43c2QKBVERAAB4esRFAOgjHtm0K1+8eVmue2BDOvY3xZw2dVje9rzpOX3aMFERAAA4aOIiAPRyD65ryBdufjS/XLSp87Gzjh6Rtz1veuZNGlrCZQAAQE8nLgJAL3XPqu35wk3LcusjW5IkhUJy3nGj8zdnTc+scXUlXgcAAPQG4iIA9CLFYjG/XbY1X7hpWe5auT1JUl5WyEvnjs3/O3NaZowaVOKFAABAbyIuAkAvUCwWc+Pizfn8zcvywNqdSZLK8kJeMW983vLcaZk0bEBpBwIAAL2SuAgAPVh7RzE/f3Bjvnjzsiyp35Ukqa4oy2tOmZg3P2dqxg7uV+KFAABAbyYuAkAP1NrekR/dtz7/dcvyrNi6J0kyoKo8f3na5LzxjCkZMai6xAsBAIC+QFwEgB6kqbU931uwLlfcsjzrd+5LktT1q8xlz5qcS0+fnMH9q0q8EAAA6EvERQDoAfa2tOWau9bkK7etyOZdzUmS4QOr8lfPnppLTp2UgdX+kw4AABx5/iQCAN1YY1Nrvn7Hqnzt9lXZvqclSTKmriZ//ZypefUpE1NTWV7ihQAAQF8mLgJAN7R9T0u+9tuVufrOVdnV1JYkmTSsf9763Gm58MTxqaooK/FCAAAAcREAupVNjU3579tW5Ft3rcm+1vYkyYyRA/O2503Pi2ePSUW5qAgAAHQf4iIAdANrt+/Nl29bnu/esy4t7R1JklnjavO2s2bkBceOSllZocQLAQAAHk9cBIAS2rBzX/7j14/m2nvXpa2jmCQ5adKQvO150/Pco0akUBAVAQCA7ktcBIAS2LGnJV+6ZVmuvnN1Wtr2n6l4xvThedvzpmf+lKGiIgAA0CN02Rs3feITn8jpp5+e/v37Z/DgwU/rNcViMR/60IcyZsyY9OvXL+ecc04effTRrpoIAEfc3pa2fPHmZXnOp27Of/9mZVraOnLKlKG59q2n5Zt/NT+nTh0mLAIAAD1Gl8XFlpaWvPKVr8xb3/rWp/2aT33qU/nP//zPXHHFFbnrrrsyYMCAnHvuuWlqauqqmQBwRLS2d+Qbv1ud5376lnz6l0uzq7ktx4ypzZWXnZz/ffOpmTdpaKknAgAAHLRCsVgsduU3uOqqq/L3f//32blz51MeVywWM3bs2LzrXe/Ku9/97iRJQ0NDRo0alauuuiqvfvWrn9b3a2xsTF1dXRoaGlJbW/tM5wPAM9LRUczPHtyYz/5qaVZt25skmTC0X979gqNz/pyxbtQCAAB0OwfT17rNey6uXLky9fX1Oeecczofq6ury/z583PnnXc+aVxsbm5Oc3Nz59eNjY1dvhUAno7fPLoln7x+SR5av/+/TcMHVuVvnzcjrzllYqoquuziAQAAgCOm28TF+vr6JMmoUaMOeHzUqFGdzz2Ryy+/PB/5yEe6dBsAHIwH1u7Mp365JLcv25YkGVhdkTc9e2r+6tlTMqC62/ynFwAA4Bk7qNMm3vve96ZQKDzlx5IlS7pq6xN63/vel4aGhs6PtWvXHtHvDwB/sHzL7vy/by3IBV+8Pbcv25aq8rK84VlTcus/nJm3nzNDWAQAAHqdg/pTzrve9a5ceumlT3nM1KlTD2nI6NGjkySbNm3KmDFjOh/ftGlTjj/++Cd9XXV1daqrqw/pewLA4VDf0JT/uPGRfPf369LeUUyhkLz8hHF5xzlHZcLQ/qWeBwAA0GUOKi6OGDEiI0aM6JIhU6ZMyejRo3PjjTd2xsTGxsbcddddB3XHaQA4Uhr2tua/bl2eK29fmea2jiTJOceMzLvPPTozR7upGAAA0Pt12fVZa9asyfbt27NmzZq0t7fn/vvvT5JMnz49AwcOTJLMnDkzl19+eV7+8penUCjk7//+7/Pxj388M2bMyJQpU/LBD34wY8eOzcte9rKumgkAB62ptT1X3bEqX7p5WRqb2pIkJ00akve8cGZOnjy0xOsAAACOnC6Lix/60Idy9dVXd359wgknJEluvvnmnHnmmUmSpUuXpqGhofOYf/zHf8yePXvy5je/OTt37swZZ5yR66+/PjU1NV01EwCetrb2jnxvwbp87tePZFNjc5LkqFED84/nzszZx4xMoVAo8UIAAIAjq1AsFoulHnE4NTY2pq6uLg0NDamtdUkaAM9csVjM9Q/V59O/WpoVW/YkScYN7pd3PP+ovPyEcSkvExUBAIDe42D6mttWAsBTuGPZ1nzy+iV5YN3+M+2H9K/M2543I5ecOjHVFeUlXgcAAFBa4iIAPIGH1jfkk9cvyW8e3Zok6V9Vnr86Y0re9JypGVRTWeJ1AAAA3YO4CAB/YtXWPfnsDY/kugc2JEkqywu5+JSJedvzZmTEoOoSrwMAAOhexEUASLJ5V1M+f+OyfPvuNWnr2P92xBccPzbvev7RmTisf4nXAQAAdE/iIgB9WmNTa75y64p89bcrs6+1PUly5tEj8g/nHp3jxtaVeB0AAED3Ji4C0Cc1tbbnm79bnS/evCw79rYmSY6fMDjvfeHMnDp1WInXAQAA9AziIgB9SntHMdfeuy6fu+GRbGhoSpJMGzEg/3DuzJx73KgUCoUSLwQAAOg5xEUA+oRisZgbHt6UT/9yaR7dvDtJMqauJu8456hceOK4VJSXlXghAABAzyMuAtDr3b1yez55/ZIsWL0jSVLXrzJ/c9a0vO60yampLC/xOgAAgJ5LXASg11q8sTGf/uXS3LRkc5KkprIsb3jWlPz1c6elrl9lidcBAAD0fOIiAL3O2u178283PJIf3b8+xWJSXlbIq0+ekLefPSMja2tKPQ8AAKDXEBcB6DW27m7OF25alm/dtTqt7cUkyYvnjMm7X3B0pgwfUOJ1AAAAvY+4CECPt7u5Lf9924r8z29WZE9Le5Lk2TOG5x/PnZnZ4+tKvA4AAKD3EhcB6LGa29pzzV1r8oWblmXbnpYkyZzxdXnPeTPzrOnDS7wOAACg9xMXAehx2juK+fH96/NvNzySdTv2JUmmDB+Qd7/g6Lxo9ugUCoUSLwQAAOgbxEUAeoxisZibl27Op65fmiX1u5IkIwdV5+3nzMirTpqQyvKyEi8EAADoW8RFAHqEBat35JO/WJK7V21Pkgyqqchbz5yWy06fkn5V5SVeBwAA0DeJiwB0a5t3NeVff74kP7hvfZKkuqIsl54+OW89c1oG968q8ToAAIC+TVwEoFtqa+/I1XeuzudueCS7mttSKCSvnDc+73j+URlT16/U8wAAAIi4CEA3dNeKbfnQjxdl6ab976s4d3xdPnrBrMydMLi0wwAAADiAuAhAt7G5sSn/8vPF+dH9G5IkQ/pX5h/Pm5m/OGlCysrcARoAAKC7ERcBKLnW9o5cfceqfO7Xj2b3Y5dAX3zKxLz7BUdnyADvqwgAANBdiYsAlNTvVmzLh378UB7ZtDtJMnfC4HzsguMyZ/zg0g4DAADgzxIXASiJTY1N+cTPFucnD/zxEuj3vnBmXjnPJdAAAAA9hbgIwBHV2t6Rq25flc/9+pHsaWlPoZC8dv7+S6AH93cJNAAAQE8iLgJwxNyxfGs+/ONFeXTz/kugT5g4OB+7YFZmjasr8TIAAAAOhbgIQJerb2jKx3/2cH66cGOSZOiAqrz3hTPzihPHuwQaAACgBxMXAegyLW0dufL2lfmPGx/N3pb2lBWSS06dlHc9/+jU9a8s9TwAAACeIXERgC5x+7Kt+dCPH8ryLXuSJCdOHJyPugQaAACgVxEXATisNjbsy8d/tjg/e+wS6GGPXQJ9kUugAQAAeh1xEYDDoqWtI1/97cp8/qY/XgL9utMm5x3PPyp1/VwCDQAA0BuJiwA8Y795dEs+/JNFWfHYJdAnTRqSj1xwXI4b6xJoAACA3kxcBOCQbdi5Lx//2cP5+YP1SZLhA6vyvhcekwtPHJdCwSXQAAAAvZ24CMBBa25rz//8ZmW+cNOy7Gt1CTQAAEBfJS4CcFBue2RL/vkni7Ji6/5LoE+ePCQfvWBWjhlTW+JlAAAAHGniIgBPy/qd+/Kx6x7O9Yv+cAl0dd7/opl5+QkugQYAAOirxEUAntIfLoH+/E2Ppqm1I+Vlhbz+tMn5++fPSG2NS6ABAAD6MnERgCd1y9LN+ch1D2flY5dAnzJ5aD76suMyc7RLoAEAABAXAXgC63bszcd++nB+uWhTkmTEoOr804uOyQXHj3UJNAAAAJ3ERQA6NbW2579vW5Ev3rKs8xLoy06fnLefMyODXAINAADA/yEuApAkuXnp5vzzTxZl9ba9SZL5U4bmoxfMytGjB5V4GQAAAN2VuAjQx63dvjcf/enDueHh/ZdAjxxUnX968TF56VyXQAMAAPDUxEWAPqqptT1fuW1FvnjzsjS3daSirJDLnjU5f3e2S6ABAAB4esRFgD7opiWb8pHrHu68BPq0qcPy0QuOy4xRLoEGAADg6RMXAfqQNdv25qM/XZRfL96cJBlVW50PvPjYvGTOGJdAAwAAcNDERYA+oKm1PVfcujxfumV5Wh67BPqNZ0zJ3549IwOr/acAAACAQ+NPlAC93K8f3pSP/HRR1m7flyQ5fdr+S6Cnj3QJNAAAAM+MuAjQS63eticfue7h3LRk/yXQo2tr8oGXHJMXz3YJNAAAAIeHuAjQyzS1tudLtyzPFbfuvwS6sryQN54xNX/7vOkZ4BJoAAAADiN/ygToJYrFYm54eFM++tOHs27H/kugz5g+PP/80uMyfeTAEq8DAACgNxIXAXqBbbub88EfP5SfP1ifJBlTV5MPvuTYvHDWaJdAAwAA0GXERYAe7leL6vP+Hz6YrbtbUlFWyJues/8S6P5V/hUPAABA1/InT4AeqmFfaz5y3aL84N71SZKjRg3Mv73q+MwaV1fiZQAAAPQV4iJAD/SbR7fkH7+/MBsbmlIoJG9+ztS88/lHpbqivNTTAAAA6EPERYAeZE9zWy7/xeJ883drkiSTh/XPZ181N/MmDS3xMgAAAPoicRGgh7hn1fa8+3sPZPW2vUmS1582Ke954UzvrQgAAEDJ+BMpQDfX1Nqef7vhkfz3b1akWEzG1tXkU6+YmzNmDC/1NAAAAPo4cRGgG3twXUPe+d378+jm3UmSV84bnw+ef2xqaypLvAwAAADERYBuqbW9I1+4aVm+cPOytHcUM3xgdS6/cHaef+yoUk8DAACATuIiQDeztH5X3vnd+7NoQ2OS5MVzxuRjF8zK0AFVJV4GAAAABxIXAbqJ9o5i/vs3K/Jvv3okLe0dGdy/Mh+7YFbOnzu21NMAAADgCYmLAN3Aqq178q7vPZAFq3ckSZ43c2T+9cLZGVlbU+JlAAAA8OTERYAS6ugo5pt3rc7lP1+Sfa3tGVhdkQ+95Ni88qTxKRQKpZ4HAAAAT0lcBCiR9Tv35T3fX5jfLtuaJDlt6rB8+pVzMn5I/xIvAwAAgKdHXAQ4worFYr6/YF0+et3D2dXclprKsrz3vJl53WmTU1bmbEUAAAB6DnER4AjavKsp7//BQ/n14k1JkhMmDs5nXzk3U0cMLPEyAAAAOHjiIsAR8rOFG/OBHz2YHXtbU1Velnc8/6i8+TlTU+5sRQAAAHoocRGgi+3c25IP/nhRrntgQ5Lk2DG1+be/mJuZo2tLvAwAAACeGXERoAvdtGRT3nPtg9myqznlZYX8zZnT8rbnzUhVRVmppwEAAMAzJi4CdIFdTa352E8fznd/vy5JMm3EgPzbq47P3AmDSzsMAAAADiNxEeAwu2PZ1vzD9xdm/c59KRSSNz5rSt597tGpqSwv9TQAAAA4rMRFgMNkX0t7Pnn9klx1x6okyYSh/fKZV8zN/KnDSjsMAAAAuoi4CHAYLFi9I+/+3gNZuXVPkuS18yfm/S86JgOq/WsWAACA3sufegGegea29nzu14/my7cuT0cxGV1bk0++Yk6ee9SIUk8DAACALicuAhyiRRsa8q7vPpAl9buSJBeeMC4fPv+41PWvLPEyAAAAODLERYCD1Nbekf+6ZXn+48ZH09ZRzLABVfnEy2flvFljSj0NAAAAjihxEeAgLNu8O+/67v15YF1DkuTc40blEy+fneEDq0u8DAAAAI48cRHgaejoKOZrt6/Mp3+5NM1tHamtqchHL5iVC44fm0KhUOp5AAAAUBLiIsCfsWbb3rz7+w/k7pXbkyTPPWpEPnnRnIyuqynxMgAAACgtcRHgSRSLxVxz95p84meLs7elPf2ryvOBFx+b15wywdmKAAAAEHER4AnVNzTlH69dmNse2ZIkOWXK0HzmFXMzcVj/Ei8DAACA7kNcBPgTxWIxP7p/fT7840VpbGpLVUVZ/vHco/OGZ01JWZmzFQEAAOBPiYsAj9m6uzkf+OFDuX5RfZJk7vi6fPZVczN95KASLwMAAIDuSVwESHL9Q/X5px8+mG17WlJRVsjbz56Rt545LRXlZaWeBgAAAN2WuAj0aQ17W/PP1y3KD+9bnySZOXpQPvuquTlubF2JlwEAAED3Jy4Cfdatj2zJe76/MPWNTSkrJG957rS8/ZwZqa4oL/U0AAAA6BHERaDP2dPclk/8fHGuuWtNkmTq8AH5zKvm5sSJQ0q8DAAAAHoWcRHoUxZvbMxbv7kgq7btTZJcevrkvOe8melX5WxFAAAAOFjiItBn/Pj+9XnPtQvT1NqRcYP75dOvnJPTpw0v9SwAAADoscRFoNdrbe/I5T9fkq/dvjJJ8uwZw/Ofrz4hQwZUlXgZAAAA9GziItCrbdnVnL+55t7cvXJ7kuRvzpqWdz7/6JSXFUq8DAAAAHq+sq76iT/xiU/k9NNPT//+/TN48OCn9ZpLL700hULhgI/zzjuvqyYCvdy9a3bkJZ//Te5euT0DqytyxSXz8g/nzhQWAQAA4DDpsjMXW1pa8spXvjKnnXZavvrVrz7t15133nm58sorO7+urq7uinlAL1YsFnPN3Wvyzz9ZlNb2YqaNGJAv/+VJmT5yYKmnAQAAQK/SZXHxIx/5SJLkqquuOqjXVVdXZ/To0V2wCOgLmlrb86EfP5Tv/n5dkuS840bnM6+am4HV3gUCAAAADrdu96ftW265JSNHjsyQIUPyvOc9Lx//+MczbNiwJz2+ubk5zc3NnV83NjYeiZlAN7R+57689ZsLsnBdQ8oKyT+cOzNvee7UFAougwYAAICu0K3i4nnnnZcLL7wwU6ZMyfLly/P+978/L3zhC3PnnXemvLz8CV9z+eWXd54lCfRddyzbmrd9+75s39OSIf0r85+vOSHPnjGi1LMAAACgVzuoG7q8973vfdwNV/7vx5IlSw55zKtf/eq89KUvzezZs/Oyl70sP/3pT3PPPffklltuedLXvO9970tDQ0Pnx9q1aw/5+wM9T7FYzFduW55LvnpXtu9pyaxxtfnJ284QFgEAAOAIOKgzF9/1rnfl0ksvfcpjpk6d+kz2PO7nGj58eJYtW5azzz77CY+prq520xfoo/Y0t+Ufv78wP3twY5LkohPH5xMvn5Wayic+0xkAAAA4vA4qLo4YMSIjRhy5s4HWrVuXbdu2ZcyYMUfsewI9w4otu/PX31iQRzfvTmV5IR86/7hcMn+i91cEAACAI+igLos+GGvWrMn999+fNWvWpL29Pffff3/uv//+7N69u/OYmTNn5oc//GGSZPfu3fmHf/iH/O53v8uqVaty44035oILLsj06dNz7rnndtVMoAe64eFNueALt+fRzbszclB1vvPmU/OXp04SFgEAAOAI67IbunzoQx/K1Vdf3fn1CSeckCS5+eabc+aZZyZJli5dmoaGhiRJeXl5Fi5cmKuvvjo7d+7M2LFj84IXvCAf+9jHXPYMJEnaO4r5j18/kv+8aVmS5OTJQ/LF156YkYNqSrwMAAAA+qZCsVgslnrE4dTY2Ji6uro0NDSktra21HOAw2Tn3pb8/f/en1uWbkmSXHr65PzTi49JZXmXnYANAAAAfdLB9LUuO3MR4HB5eENj3vLNBVmzfW9qKsty+YWz8/ITxpd6FgAAAPR54iLQrf3ovvV57w8Wpqm1IxOG9ssVl8zLcWPrSj0LAAAAiLgIdFOt7R35l58vzpW3r0qSPOeoEfnPVx+fwf2rSjsMAAAA6CQuAt3O5l1Nedu37svdq7YnSd521vS84/lHpbzM3aABAACgOxEXgW5lweod+X/fWpBNjc0ZWF2Rf3vV3LzguNGlngUAAAA8AXER6BaKxWK+ddeafOS6RWltL2b6yIH58l/Oy7QRA0s9DQAAAHgS4iJQck2t7fngjx7K9xasS5K8aPbofOoVczOw2r+iAAAAoDvzJ3egpNbt2Ju3fvPePLi+IWWF5D3nzcybnzM1hYL3VwQAAIDuTlwESua3j27N33773uzY25oh/Svz+decmDNmDC/1LAAAAOBpEheBI65YLObLt63Ip65fko5iMntcXf7rkhMzfkj/Uk8DAAAADoK4CBxRu5vb8g/feyC/eKg+SfLKeePzsZfNSk1leYmXAQAAAAdLXASOmOVbduevv7EgyzbvTmV5IR8+/7i8dv5E768IAAAAPZS4CBwRv1xUn3d994Hsbm7LqNrqfOm18zJv0pBSzwIAAACeAXER6FLtHcX8+w2P5As3L0uSnDJ5aL7w2hMyclBNiZcBAAAAz5S4CHSZnXtb8nffuT+3PbIlSXLZsybn/S86JpXlZSVeBgAAABwO4iLQJRZtaMhbvrkga7fvS01lWf71wjl52QnjSj0LAAAAOIzEReCw++F96/K+HzyYptaOTBjaL1++5KQcO7a21LMAAACAw0xcBA6b1vaOfOJni3PVHauSJM89akT+49XHZ3D/qtIOAwAAALqEuAgcFpt3NeVvvnVv7lm1I0nyd8+bnrefc1TKywolXgYAAAB0FXEReMYWrN6Rt35zQTbvas6g6or8218cn+cfO6rUswAAAIAuJi4Ch6xYLOabd63JR69blNb2YmaMHJgv/+W8TB0xsNTTAAAAgCNAXAQOSVNrez7wo4fy/QXrkiQvnj0mn3rFnAyo9q8VAAAA6CtUAOCgrd2+N2/91oI8tL4xZYXkvS+cmTc9e2oKBe+vCAAAAH2JuAgclN88uiV/9+37smNva4YOqMoXXnNCTp8+vNSzAAAAgBIQF4GnpVgs5opbV+TTv1ySjmIyZ3xd/uuSeRk3uF+ppwEAAAAlIi4Cf9bu5rb8w/ceyC8eqk+SvOqk8fnoBbNSU1le4mUAAABAKYmLwFNauXVP/urqe7J8y55UlhfykZfOymtOmeD9FQEAAABxEXhyD29ozOu+dle27m7J6NqafOmSE3PixCGlngUAAAB0E+Ii8IQWrN6Ry668O41NbTl2TG2uesPJGTmoptSzAAAAgG5EXAQe57ePbs2bv/H77G1pz7xJQ/K1S09OXb/KUs8CAAAAuhlxETjArxbV523X3JeW9o48e8bwfPkv56V/lX9VAAAAAI+nGACdfnjfurz7ewvT3lHMeceNzn+85vhUV7gjNAAAAPDExEUgSfKNO1flgz9elCS56MTx+eRFs1NRXlbiVQAAAEB3Ji4C+dIty/Kp65cmSS49fXI+9JJjU1ZWKPEqAAAAoLsTF6EPKxaL+eT1S3PFrcuTJH/7vOl55/OPSqEgLAIAAAB/nrgIfVRHRzEf+slD+ebv1iRJ3v+imXnzc6aVeBUAAADQk4iL0Ae1tnfkH773QH50/4YUCsknXjY7F8+fWOpZAAAAQA8jLkIf09Tanrddc19+vXhTKsoK+eyr5uaC48eVehYAAADQA4mL0IfsaW7Lm77++9yxfFuqKsryX689MWcfM6rUswAAAIAeSlyEPmLn3pZceuU9uX/tzgyoKs9/v/6knD5teKlnAQAAAD2YuAh9wOZdTXndV+/OkvpdGdy/MldddkqOnzC41LMAAACAHk5chF5u3Y69ueR/7sqqbXszYlB1vvnG+Tl69KBSzwIAAAB6AXERerHlW3bnL//nrmxoaMr4If3yrb+an0nDBpR6FgAAANBLiIvQSy3a0JDXffXubNvTkmkjBuSbfzU/Y+r6lXoWAAAA0IuIi9ALLVi9PZdeeU92NbXluLG1+fobTsmwgdWlngUAAAD0MuIi9DK/fXRr3vT132dfa3tOnjwkX7305NTWVJZ6FgAAANALiYvQi/xyUX3+9pr70tLekeccNSJfvmRe+lWVl3oWAAAA0EuJi9BL/ODedfmH7y9Me0cxL5w1Op979fGprhAWAQAAgK4jLkIv8I07V+WDP16UJHnFvPH51wtnp6K8rMSrAAAAgN5OXIQerFgs5ku3LM+nf7k0SXLp6ZPzoZccm7KyQomXAQAAAH2BuAg9VLFYzL9evyRfvnVFkuTvzp6Rd5wzI4WCsAgAAAAcGeIi9EDtHcV88McP5Zq71iRJPvDiY/JXz55a4lUAAABAXyMuQg/T2t6Rd3/vgfz4/g0pFJLLXz47rz5lYqlnAQAAAH2QuAg9SFNre952zb359eLNqSgr5N//4vicP3dsqWcBAAAAfZS4CD3E7ua2vOnq3+fOFdtSXVGW/7rkxDxv5qhSzwIAAAD6MHEReoCde1ty6ZX35P61OzOgqjxfvfTknDp1WKlnAQAAAH2cuAjd3OZdTXndV+/OkvpdGdy/MldfdkrmThhc6lkAAAAA4iJ0Z+t27M0l/3NXVm3bm5GDqvONN87P0aMHlXoWAAAAQBJxEbqt5Vt255L/uSsbG5oyfki/fOuv5mfSsAGlngUAAADQSVyEbmjRhoa87qt3Z9uelkwfOTDffOP8jK6rKfUsAAAAgAOIi9DN/H7V9lx21T3Z1dSWWeNqc/Vlp2TYwOpSzwIAAAB4HHERupHbHtmSv/7Gguxrbc8pk4fmfy49KbU1laWeBQAAAPCExEXoJq5/aGP+7tv3p6W9I889akSuuGRe+lWVl3oWAAAAwJMSF6Eb+P6CdfnH7z+QjmLy4tlj8u9/cXyqKspKPQsAAADgKYmLUGJX37EqH/7JoiTJq04an8svnJPyskKJVwEAAAD8eeIilEixWMwXb16Wz/zqkSTJG541JR948TEpExYBAACAHkJchBIoFov5118syZdvW5EkefvZM/L358xIoSAsAgAAAD2HuAhHWHtHMR/88UO55q41SZIPvPiY/NWzp5Z4FQAAAMDBExfhCGpt78i7vvtAfvLAhhQKyb9eODt/cfLEUs8CAAAAOCTiIhwhTa3t+Ztv3Zsbl2xORVkhn3v18XnJnLGlngUAAABwyMRFOAJ2N7flTVf/Pneu2JbqirJcccm8nDVzZKlnAQAAADwj4iJ0sZ17W/L6K+/JA2t3ZmB1Rb76+pMyf+qwUs8CAAAAeMbERehCmxub8pdfvTtLN+3KkP6VufoNp2TO+MGlngUAAABwWIiL0EV2NbXmdV/bHxZHDqrOt/5qfmaMGlTqWQAAAACHjbgIXaCtvSNvu+a+LKnflRGDqvP9t5yeicP6l3oWAAAAwGFVVuoB0NsUi8V85LqHc+sjW1JTWZavvv4kYREAAADolcRFOMy+dvuqfON3q1MoJP/x6hO8xyIAAADQa4mLcBjd8PCmfPxnDydJ3v/CY3LucaNLvAgAAACg64iLcJg8tL4hf/ft+1IsJhfPn5i/evaUUk8CAAAA6FLiIhwGGxv25Y1X35N9re159ozh+chLj0uhUCj1LAAAAIAuJS7CM7S7uS1vuOr32dTYnKNGDcwXX3tiKsv9owUAAAD0fgoIPANt7R35u2/fl8UbGzN8YHW+dunJqa2pLPUsAAAAgCNCXIRn4OM/W5yblmxOTWVZ/uf1J2X8kP6lngQAAABwxIiLcIiuvH1lrrpjVZLk3191fI6fMLikewAAAACONHERDsGNizflYz99OEny3hfOzAtnjynxIgAAAIAjT1yEg/TQ+ob87bfvS0cxefXJE/LXz5la6kkAAAAAJSEuwkGob2jKG6++J3tb2nPG9OH52MtmpVAolHoWAAAAQEmIi/A07WluyxuvviebGpszY+TAfPG1J6ay3D9CAAAAQN+ljMDT0N5RzNu/c18WbWjMsAFV+dqlJ6euX2WpZwEAAACUlLgIT8PHf/Zwfr14c6oryvLfrz8pE4b2L/UkAAAAgJLrsri4atWqvPGNb8yUKVPSr1+/TJs2LR/+8IfT0tLylK9ramrK3/zN32TYsGEZOHBgLrroomzatKmrZsKf9fU7V+XK21clSf7tVcfnxIlDSjsIAAAAoJvosri4ZMmSdHR05Mtf/nIWLVqUf//3f88VV1yR97///U/5une84x257rrr8r3vfS+33nprNmzYkAsvvLCrZsJTunnJ5vzzTxYlSf7h3KPz4jljSrwIAAAAoPsoFIvF4pH6Zp/+9KfzX//1X1mxYsUTPt/Q0JARI0bkmmuuySte8Yok+yPlMccckzvvvDOnnnrqn/0ejY2NqaurS0NDQ2praw/rfvqWhzc05pVX3JE9Le151Unj88mL5rgzNAAAANDrHUxfO6LvudjQ0JChQ4c+6fMLFixIa2trzjnnnM7HZs6cmYkTJ+bOO+88EhMhSbKpsSlvvPqe7Glpz2lTh+XjL5stLAIAAAD8HxVH6hstW7Ysn//85/OZz3zmSY+pr69PVVVVBg8efMDjo0aNSn19/RO+prm5Oc3NzZ1fNzY2Hpa99F17W9ryxqvvycaGpkwbMSBXXDIvVRXufQQAAADwfx10MXnve9+bQqHwlB9Lliw54DXr16/Peeedl1e+8pV505vedNjGJ8nll1+eurq6zo8JEyYc1p+fvqW9o5i3f+f+PLS+MUMHVOXKS09JXf/KUs8CAAAA6JYO+szFd73rXbn00kuf8pipU6d2/njDhg0566yzcvrpp+crX/nKU75u9OjRaWlpyc6dOw84e3HTpk0ZPXr0E77mfe97X975znd2ft3Y2Cgwcsgu//ni3PDwplRVlOW/XzcvE4f1L/UkAAAAgG7roOPiiBEjMmLEiKd17Pr163PWWWdl3rx5ufLKK1NW9tQnSs6bNy+VlZW58cYbc9FFFyVJli5dmjVr1uS00057wtdUV1enurr64H4R8AS++bvV+Z/frkySfOaVczNv0pO/PygAAAAAXXhDl/Xr1+fMM8/MxIkT85nPfCZbtmxJfX39Ae+duH79+sycOTN33313kqSuri5vfOMb8853vjM333xzFixYkMsuuyynnXba07pTNByqWx/Zkg//ZFGS5N0vOCovnTu2xIsAAAAAur8uu6HLDTfckGXLlmXZsmUZP378Ac8Vi8UkSWtra5YuXZq9e/d2Pvfv//7vKSsry0UXXZTm5uace+65+dKXvtRVMyFL6hvzN9+6N+0dxVx04vj8zVnTSz0JAAAAoEcoFP9Q+nqJxsbG1NXVpaGhIbW1taWeQze3eVdTXv7FO7J+576cOnVovv6G+e4MDQAAAPRpB9PXVBT6rH0t7XnT1b/P+p37MnX4gFxxyTxhEQAAAOAgKCn0SR0dxbzjf+/PA+saMqR/Za687OQM7l9V6lkAAAAAPYq4SJ/0yeuX5PpF9akqL8tXXndSJg0bUOpJAAAAAD2OuEifc81da/Ll21YkST79yjk5efLQEi8CAAAA6JnERfqU2x7Zkg/++KEkyTvOOSoXHD+uxIsAAAAAei5xkT5jaf2u/M237k17RzEXnjAuf3f29FJPAgAAAOjRxEX6hC27mvOGq+7Jrua2nDJlaC6/aHYKhUKpZwEAAAD0aOIivd6+lvb81dd/n/U792XK8AH58iXzUl1RXupZAAAAAD2euEiv1tFRzDu/e38eWLszg/tX5muXnpwhA6pKPQsAAACgVxAX6dU+9cul+cVD9aksL+TLl8zLlOEDSj0JAAAAoNcQF+m1vnP3mlxx6/IkyadeMSfzpw4r8SIAAACA3kVcpFe6fdnWfOBHDyVJ/u7sGXn5CeNLvAgAAACg9xEX6XUe3bQrb/nmgrR1FHPB8WPzjnNmlHoSAAAAQK8kLtKrbN3dnMuuuie7mtpy0qQh+eRFc1IoFEo9CwAAAKBXEhfpNZpa2/Omr/8+63bsy6Rh/fOV152UmsryUs8CAAAA6LXERXqFjo5i3vW9B3Lfmp2p61eZr116coYOqCr1LAAAAIBeTVykV/jsDUvzs4UbU1leyBWXzMu0EQNLPQkAAACg1xMX6fG++/u1+eLNy5Mkl184J6dNG1biRQAAAAB9g7hIj3bH8q15/w8eTJK87azpecW88SVeBAAAANB3iIv0WMs2785bvrEgbR3FvGTOmLzz+UeVehIAAABAnyIu0iNt292cN1x1Txqb2nLixMH5zCvnpqysUOpZAAAAAH2KuEiP09Tanjd/Y0HWbN+bCUP75b9fd1JqKstLPQsAAACgzxEX6VGKxWL+8fsLs2D1jgyqqciVl56cYQOrSz0LAAAAoE8SF+lRvnTL8vzkgQ2pKCvky5fMy/SRg0o9CQAAAKDPEhfpMR7ZtCuf+/UjSZKPv2xWTp8+vMSLAAAAAPo2cZEeob1j/+XQre3FnHPMyPzFyRNKPQkAAACgzxMX6RGuvmNV7l+7MwOrK/Kxl81KoeDO0AAAAAClJi7S7a3dvjef/uXSJMl7XzgzY+r6lXgRAAAAAIm4SDdXLBbz/h8+mH2t7TllytBcfMrEUk8CAAAA4DHiIt3aD+5dn988ujVVFWX51wtnp6zM5dAAAAAA3YW4SLe1dXdzPvazh5Mkbz97RqaOGFjiRQAAAAD8KXGRbuuff7IoO/e25tgxtXnzc6aWeg4AAAAA/4e4SLf064c35acLN6askHzyojmpLPe3KgAAAEB3o9jQ7exqas0HfvRQkuRNz56a2ePrSrwIAAAAgCciLtLtfPL6JalvbMqkYf3z9+ccVeo5AAAAADwJcZFu5e6V2/PN361Jklx+4ez0qyov8SIAAAAAnoy4SLfR1Nqe9167MEny6pMn5PRpw0u8CAAAAICnIi7SbXz+pkezYuuejBxUnfe96JhSzwEAAADgzxAX6RYe3tCYL9+6Ikny0Qtmpa5fZYkXAQAAAPDniIuUXFt7R95z7cK0dRRz3nGjc96s0aWeBAAAAMDTIC5Scl+7fWUeXN+Q2pqKfPSC40o9BwAAAICnSVykpFZv25N/u+GRJMk/vfiYjKytKfEiAAAAAJ4ucZGSKRaLed8PHkxTa0dOnzYsrzppQqknAQAAAHAQxEVK5ru/X5s7lm9LTWVZLr9wdgqFQqknAQAAAHAQxEVKYnNjUz7+s8VJknc+/6hMGjagxIsAAAAAOFjiIiXx4Z8syq6mtsweV5c3PGtKqecAAAAAcAjERY646x+qzy8eqk9FWSGfvGhOKsr9bQgAAADQE6k6HFEN+1rzoR8/lCT56+dOzbFja0u8CAAAAIBDJS5yRF3+88XZvKs5U0cMyN8+b0ap5wAAAADwDIiLHDF3LN+a79yzNknyrxfOSU1leYkXAQAAAPBMiIscEfta2vO+HzyYJLnk1Ik5ZcrQEi8CAAAA4JkSFzkiPvfrR7J6296Mrq3Je86bWeo5AAAAABwG4iJd7sF1Dfnv36xIknz8ZbMyqKayxIsAAAAAOBzERbpUa3tH/vHahekoJi+ZMybnHDuq1JMAAAAAOEzERbrUV25bkcUbGzO4f2X++aXHlXoOAAAAAIeRuEiXWb5ld/7jxkeTJB988bEZPrC6xIsAAAAAOJzERbpER0cx7/vBg2lp68izZwzPhSeOK/UkAAAAAA4zcZEu8e171uTuldvTv6o8//Ly2SkUCqWeBAAAAMBhJi5y2NU3NOVff74kSfLuFxydCUP7l3gRAAAAAF1BXOSwKhaL+cCPHsqu5rYcP2FwXn/65FJPAgAAAKCLiIscVj97cGN+vXhTKssL+dQr5qS8zOXQAAAAAL2VuMhhs2NPS/75J4uSJP/vzOk5atSgEi8CAAAAoCuJixw2H//Z4mzd3ZIZIwfm/501rdRzAAAAAOhi4iKHxW2PbMm1965LoZD860VzUl1RXupJAAAAAHQxcZFnbE9zW97/wweTJK8/bXLmTRpS4kUAAAAAHAniIs/YZ3/1SNbt2Jdxg/vlH849utRzAAAAADhCxEWekfvW7MiVd6xMknzi5bMyoLqixIsAAAAAOFLERQ5ZS1tH3nvtgykWk5efMC5nHj2y1JMAAAAAOILERQ7ZFbcuz9JNuzJ0QFU++JJjSz0HAAAAgCNMXOSQLNu8K1+4aVmS5MPnH5uhA6pKvAgAAACAI01c5KB1dBTznmsfTEt7R543c2ReOndsqScBAAAAUALiIgftG79bnQWrd2RgdUU+/rJZKRQKpZ4EAAAAQAmIixyU9Tv35VPXL0mSvOe8ozN2cL8SLwIAAACgVMRFnrZisZh/+uGD2dPSnpMnD8lr508q9SQAAAAASkhc5Gn78f0bcsvSLakqL8vlF85JWZnLoQEAAAD6MnGRp2Xb7uZ85LpFSZK/O3t6po8cWOJFAAAAAJSauMjT8tGfPpwde1szc/Sg/PVzp5V6DgAAAADdgLjIn3XTkk358f0bUlZIPvWKOaks97cNAAAAAOIif8bu5rZ84IcPJUneeMaUzBk/uLSDAAAAAOg2xEWe0qevX5INDU2ZOLR/3vn8o0s9BwAAAIBuRFzkST2wdme+/rvVSZJ/efns9KsqL/EiAAAAALoTcZEn9ZXbVqRYTF5+wricMWN4qecAAAAA0M2IizyhjQ37cv2i+iTJXz93aonXAAAAANAdiYs8oW/+bnXaO4o5derQzBxdW+o5AAAAAHRD4iKP09Tanm/fvTZJcunpk0s7BgAAAIBuS1zkcX66cGO272nJ2LqanHPMqFLPAQAAAKCbEhc5QLFYzFV3rEyS/OVpk1NR7m8RAAAAAJ6YcsQB7l2zIw+tb0x1RVleffKEUs8BAAAAoBsTFznAlbevSpK87PhxGTKgqrRjAAAAAOjWxEU6bWpsyvUP1SdJXu9GLgAAAAD8GeIinb71u9Vp6yjmlClDc+zY2lLPAQAAAKCb67K4uGrVqrzxjW/MlClT0q9fv0ybNi0f/vCH09LS8pSvO/PMM1MoFA74eMtb3tJVM3lMc1t7rrl7TZLkUmctAgAAAPA0VHTVT7xkyZJ0dHTky1/+cqZPn56HHnoob3rTm7Jnz5585jOfecrXvulNb8pHP/rRzq/79+/fVTN5zM8WbszW3S0ZU1eTFxw7qtRzAAAAAOgBuiwunnfeeTnvvPM6v546dWqWLl2a//qv//qzcbF///4ZPXp0V03j/ygWi7nqjlVJkktOnZSKclfLAwAAAPDnHdGK1NDQkKFDh/7Z4771rW9l+PDhmTVrVt73vvdl7969R2Bd33Xf2p1ZuK4hVRVlec0pE0s9BwAAAIAeosvOXPy/li1bls9//vN/9qzFiy++OJMmTcrYsWOzcOHCvOc978nSpUvzgx/84AmPb25uTnNzc+fXjY2Nh3V3X3DV7auSJBfMHZuhA6pKOwYAAACAHuOg4+J73/vefPKTn3zKYxYvXpyZM2d2fr1+/fqcd955eeUrX5k3velNT/naN7/5zZ0/nj17dsaMGZOzzz47y5cvz7Rp0x53/OWXX56PfOQjB/mr4A82Nzbl5w9uTJK83o1cAAAAADgIhWKxWDyYF2zZsiXbtm17ymOmTp2aqqr9Z8Bt2LAhZ555Zk499dRcddVVKSs7uCux9+zZk4EDB+b666/Pueee+7jnn+jMxQkTJqShoSG1tbUH9b36on+/4ZH8x42P5uTJQ/K9t5xe6jkAAAAAlFhjY2Pq6uqeVl876DMXR4wYkREjRjytY9evX5+zzjor8+bNy5VXXnnQYTFJ7r///iTJmDFjnvD56urqVFdXH/TPS9LS1pFv3bUmibMWAQAAADh4XXZDl/Xr1+fMM8/MxIkT85nPfCZbtmxJfX196uvrDzhm5syZufvuu5Mky5cvz8c+9rEsWLAgq1atyk9+8pO87nWvy3Oe85zMmTOnq6b2WT9/cGO27m7O6NqanHucu3MDAAAAcHC67IYuN9xwQ5YtW5Zly5Zl/PjxBzz3hyuxW1tbs3Tp0s67QVdVVeXXv/51Pve5z2XPnj2ZMGFCLrroonzgAx/oqpl92pV3rEqSXHLqxFSWH9EbhwMAAADQCxz0ey52dwdzTXhfdt+aHXn5l+5IVXlZ7njf8zJ8oEvLAQAAADi4vuZ0tT7q6sfOWjx/7lhhEQAAAIBDIi72QZt3NeVnD25MklzqRi4AAAAAHCJxsQ/69l1r09pezIkTB2f2+LpSzwEAAACghxIX+5iWto58867VSZJLnzWlxGsAAAAA6MnExT7mFw9tzJZdzRk5qDovnDW61HMAAAAA6MHExT7mqsdu5HLJqZNSWe4vPwAAAACHTl3qQxau25n71uxMVXlZXnPKxFLPAQAAAKCHExf7kD+ctfiSOWMyYlB1accAAAAA0OOJi33E1t3N+ekDG5Mkrz99cmnHAAAAANAriIt9xLfvWpOW9o4cP2Fw5k4YXOo5AAAAAPQC4mIf0NrekW/etTpJctmzJpd2DAAAAAC9hrjYB1z/UH02NTZnxKDqvHDWmFLPAQAAAKCXEBf7gD/cyOW18yemqsJfcgAAAAAOD6Wpl3tofUMWrN6RyvJCLp4/sdRzAAAAAOhFxMVe7g9nLb549piMHFRT2jEAAAAA9CriYi+2bXdzfvLAhiTJ60+fXNoxAAAAAPQ64mIv9p171qalrSNzx9flhIlDSj0HAAAAgF5GXOylWts78o07VydJLn3W5NKOAQAAAKBXEhd7qV8t2pT6xqYMH1iVF80eU+o5AAAAAPRC4mIvddUdK5MkF8+flOqK8hKvAQAAAKA3Ehd7oUUbGnLPqh2pKCvktfMnlnoOAAAAAL2UuNgLXX3HqiTJi2aPyajamtKOAQAAAKDXEhd7me17WvKj+zckSV5/+uTSjgEAAACgVxMXe5nv3LMmLW0dmT2uLidOHFzqOQAAAAD0YuJiL9LW3pFv3rk6SXLp6ZNTKBRKvAgAAACA3kxc7EVueHhTNjQ0ZdiAqrxk7phSzwEAAACglxMXe5ErH7uRy8XzJ6a6ory0YwAAAADo9cTFXmLxxsbcvXJ7KsoKee38SaWeAwAAAEAfIC72Elc/dtbiubNGZ3RdTWnHAAAAANAniIu9wI49LfnhfeuTJJedPrm0YwAAAADoM8TFXuB/f782zW0dOW5sbeZNGlLqOQAAAAD0EeJiD9fW3pFv3Lk6SXLp6ZNTKBRKvAgAAACAvkJc7OF+vXhz1u/cl6EDqnL+3LGlngMAAABAHyIu9nB/uJHLa06ZkJrK8tKOAQAAAKBPERd7sCX1jblzxbaUlxVyyamTSj0HAAAAgD5GXOzBrr5j/3stnnvcqIyp61fiNQAAAAD0NeJiD7Vzb0t+eN+6JMmlp08p8RoAAAAA+iJxsYf67u/Xpqm1I8eMqc3Jk4eUeg4AAAAAfZC42AO1dxTz9Tv3XxJ92emTUygUSrwIAAAAgL5IXOyBbly8Ket27MuQ/pV56fFjSz0HAAAAgD5KXOyBrr5zVZLk1adMTE1leWnHAAAAANBniYs9zCObduX2ZdtSVkguOXVSqecAAAAA0IeJiz3Mbx/dmiR5wbGjM25wvxKvAQAAAKAvqyj1AA7OG86YkuccNbzUMwAAAABAXOyJpo8cVOoJAAAAAOCyaAAAAADg0IiLAAAAAMAhERcBAAAAgEMiLgIAAAAAh0RcBAAAAAAOibgIAAAAABwScREAAAAAOCTiIgAAAABwSMRFAAAAAOCQiIsAAAAAwCERFwEAAACAQyIuAgAAAACHRFwEAAAAAA6JuAgAAAAAHBJxEQAAAAA4JOIiAAAAAHBIxEUAAAAA4JCIiwAAAADAIREXAQAAAIBDIi4CAAAAAIdEXAQAAAAADom4CAAAAAAcEnERAAAAADgk4iIAAAAAcEgqSj3gcCsWi0mSxsbGEi8BAAAAgJ7nD13tD53tqfS6uLhr164kyYQJE0q8BAAAAAB6rl27dqWuru4pjyn8//buPabq+o/j+OuAApqRIQhS4gVHWKI4KsClP5gs6KKR5lJZqGM4LZvmZV5aaUvbvM2WWFZ/YG5mSakzK5c73rZCNJJ5mZCSSoAHb0NFE4jz+f3hPIsk5Bw83M7zsZ1Nv+fzOef93V5+9vG97/l+TVNakO2I3W5XeXm5HnzwQVksltYup9Vcu3ZNvXv31p9//il/f//WLgdwC3IOT0DO4SnIOjwBOYcnIOfwBJ6Qc2OMrl+/rtDQUHl5NX5XxQ535aKXl5ceffTR1i6jzfD39++wQQfuIOfwBOQcnoKswxOQc3gCcg5P0NFzfq8rFu/ggS4AAAAAAAAAXEJzEQAAAAAAAIBLaC52UL6+vlq8eLF8fX1buxTAbcg5PAE5h6cg6/AE5ByegJzDE5Dz+jrcA10AAAAAAAAAtAyuXAQAAAAAAADgEpqLAAAAAAAAAFxCcxEAAAAAAACAS2guAgAAAAAAAHAJzcUOZPTo0QoLC5Ofn5969eql1157TeXl5Y3OuXXrlt544w316NFD3bp109ixY1VRUdFCFQPOOXv2rDIyMtSvXz916dJF4eHhWrx4sWpqahqdl5CQIIvFUu81bdq0FqoacI6rOWc9R3uzbNkyDRs2TF27dlX37t2bNGfy5Ml3recpKSnuLRRoBldybozRu+++q169eqlLly5KSkrSqVOn3Fso0ExXrlxRWlqa/P391b17d2VkZKiqqqrROezR0datW7dOffv2lZ+fn2JjY3Xo0KFGx+fk5CgyMlJ+fn6KiorSDz/80EKVtj6aix1IYmKitmzZoqKiIn377bcqLi7WK6+80uict956S999951ycnK0f/9+lZeXa8yYMS1UMeCcwsJC2e12ffrppzpx4oTWrFmj9evXa9GiRfecm5mZqfPnzzteK1asaIGKAee5mnPWc7Q3NTU1GjdunKZPn+7UvJSUlHrr+ebNm91UIdB8ruR8xYoV+uijj7R+/Xrl5eXpgQceUHJysm7duuXGSoHmSUtL04kTJ7R7927t3LlTBw4c0NSpU+85jz062qqvv/5as2fP1uLFi/Xbb79pyJAhSk5O1oULFxoc/8svv2jChAnKyMjQkSNHlJqaqtTUVB0/fryFK28dFmOMae0i4B47duxQamqqqqur1blz57vev3r1qoKCgvTll186mpCFhYUaOHCgcnNzFRcX19IlA05buXKlPvnkE/3xxx//OSYhIUHR0dH68MMPW64w4D66V85Zz9GebdiwQbNmzVJlZeU9x06ePFmVlZXavn272+sC7qem5twYo9DQUM2ZM0dz586VdHuNDw4O1oYNGzR+/PgWqBZwzsmTJ/X444/r8OHDevLJJyVJu3bt0vPPP6/S0lKFhoY2OI89Otqy2NhYPfXUU8rKypIk2e129e7dW2+++aYWLFhw1/hXX31VN27c0M6dOx3H4uLiFB0drfXr17dY3a2FKxc7qCtXrmjTpk0aNmxYg41FScrPz1dtba2SkpIcxyIjIxUWFqbc3NyWKhVolqtXryogIOCe4zZt2qTAwEANGjRICxcu1M2bN1ugOuD+uFfOWc/hSfbt26eePXvqscce0/Tp03X58uXWLgm4b86cOSObzVZvPX/ooYcUGxvLeo42Kzc3V927d3c0FiUpKSlJXl5eysvLa3Que3S0RTU1NcrPz6+3Fnt5eSkpKek/1+Lc3Nx64yUpOTnZY9buTq1dAO6v+fPnKysrSzdv3lRcXFy9rvm/2Ww2+fj43HX/l+DgYNlsNjdXCjTf6dOntXbtWq1atarRcRMnTlSfPn0UGhqqo0ePav78+SoqKtLWrVtbqFLAdU3JOes5PEVKSorGjBmjfv36qbi4WIsWLdJzzz2n3NxceXt7t3Z5QLPdWbODg4PrHWc9R1tms9nUs2fPesc6deqkgICARnPLHh1t1aVLl1RXV9fgWlxYWNjgHJvN5tFrN1cutnELFiy46ya3/379M9zz5s3TkSNH9NNPP8nb21vp6enil+9o65zNuSSVlZUpJSVF48aNU2ZmZqOfP3XqVCUnJysqKkppaWnauHGjtm3bpuLiYneeFlCPu3MOtAWu5NwZ48eP1+jRoxUVFaXU1FTt3LlThw8f1r59++7fSQD34O6cA22Fu7POHh3oOLhysY2bM2eOJk+e3OiY/v37O/4cGBiowMBARUREaODAgerdu7cOHjyo+Pj4u+aFhISopqZGlZWV9a52qaioUEhIyP06BeCenM15eXm5EhMTNWzYMH322WdOf19sbKyk21eEhYeHOz0fcIU7c856jrbC2Zw3V//+/RUYGKjTp09r5MiR9+1zgca4M+d31uyKigr16tXLcbyiokLR0dEufSbgqqZmPSQk5K6HXPz999+6cuWKU/sQ9uhoKwIDA+Xt7a2Kiop6xxvbW4eEhDg1vqOhudjGBQUFKSgoyKW5drtdklRdXd3g+zExMercubOsVqvGjh0rSSoqKlJJSUmDzUjAXZzJeVlZmRITExUTE6Ps7Gx5eTl/AXZBQYEk1du0A+7mzpyznqOtaM6+xRWlpaW6fPky6zlalDtz3q9fP4WEhMhqtTqaideuXVNeXp7TT1YHmqupWY+Pj1dlZaXy8/MVExMjSdqzZ4/sdrujYdgU7NHRVvj4+CgmJkZWq1WpqamSbvdXrFarZsyY0eCc+Ph4Wa1WzZo1y3Fs9+7dHrMX52fRHUReXp6ysrJUUFCgc+fOac+ePZowYYLCw8MdYS4rK1NkZKQOHTok6fbNoTMyMjR79mzt3btX+fn5mjJliuLj43myKNqksrIyJSQkKCwsTKtWrdLFixdls9nq3cfi3zkvLi7W+++/r/z8fJ09e1Y7duxQenq6RowYocGDB7fWqQD/yZWcs56jPSopKVFBQYFKSkpUV1engoICFRQUqKqqyjEmMjJS27ZtkyRVVVVp3rx5OnjwoM6ePSur1aqXXnpJAwYMUHJycmudBtAoZ3NusVg0a9YsLV26VDt27NCxY8eUnp6u0NBQx39wgbZm4MCBSklJUWZmpg4dOqSff/5ZM2bM0Pjx4x1PimaPjvZm9uzZ+vzzz/XFF1/o5MmTmj59um7cuKEpU6ZIktLT07Vw4ULH+JkzZ2rXrl1avXq1CgsLtWTJEv3666//2YzscAw6hKNHj5rExEQTEBBgfH19Td++fc20adNMaWmpY8yZM2eMJLN3717Hsb/++su8/vrr5uGHHzZdu3Y1L7/8sjl//nwrnAFwb9nZ2UZSg687/p3zkpISM2LECMe/jQEDBph58+aZq1evttJZAI1zJefGsJ6j/Zk0aVKDOf9nriWZ7OxsY4wxN2/eNM8++6wJCgoynTt3Nn369DGZmZnGZrO1zgkATeBszo0xxm63m3feeccEBwcbX19fM3LkSFNUVNTyxQNOuHz5spkwYYLp1q2b8ff3N1OmTDHXr193vM8eHe3R2rVrTVhYmPHx8TFPP/20OXjwoOO9//3vf2bSpEn1xm/ZssVEREQYHx8f88QTT5jvv/++hStuPRZjeNoHAAAAAAAAAOfxs2gAAAAAAAAALqG5CAAAAAAAAMAlNBcBAAAAAAAAuITmIgAAAAAAAACX0FwEAAAAAAAA4BKaiwAAAAAAAABcQnMRAAAAAAAAgEtoLgIAAAAAAADtzIEDBzRq1CiFhobKYrFo+/btTn/Gli1bFB0dra5du6pPnz5auXKl059BcxEAAAAAAABoZ27cuKEhQ4Zo3bp1Ls3/8ccflZaWpmnTpun48eP6+OOPtWbNGmVlZTn1ORZjjHGpAgAAAAAAAACtzmKxaNu2bUpNTXUcq66u1ttvv63NmzersrJSgwYN0vLly5WQkCBJmjhxompra5WTk+OYs3btWq1YsUIlJSWyWCxN+m6uXAQAAAAAAAA6mBkzZig3N1dfffWVjh49qnHjxiklJUWnTp2SdLv56OfnV29Oly5dVFpaqnPnzjX5e2guAgAAAAAAAB1ISUmJsrOzlZOTo+HDhys8PFxz587VM888o+zsbElScnKytm7dKqvVKrvdrt9//12rV6+WJJ0/f77J39XJLWcAAAAAAAAAoFUcO3ZMdXV1ioiIqHe8urpaPXr0kCRlZmaquLhYL774ompra+Xv76+ZM2dqyZIl8vJq+vWINBcBAAAAAACADqSqqkre3t7Kz8+Xt7d3vfe6desm6fZ9GpcvX64PPvhANptNQUFBslqtkqT+/fs3+btoLgIAAAAAAAAdyNChQ1VXV6cLFy5o+PDhjY719vbWI488IknavHmz4uPjFRQU1OTvorkIAAAAAAAAtDNVVVU6ffq04+9nzpxRQUGBAgICFBERobS0NKWnp2v16tUaOnSoLl68KKvVqsGDB+uFF17QpUuX9M033yghIUG3bt1y3KNx//79TtVhMcaY+31yAAAAAAAAANxn3759SkxMvOv4pEmTtGHDBtXW1mrp0qXauHGjysrKFBgYqLi4OL333nuKiorSpUuXNGrUKB07dkzGGMXHx2vZsmWKjY11qg6aiwAAAAAAAABc0vRHvwAAAAAAAADAP9BcBAAAAAAAAOASmosAAAAAAAAAXEJzEQAAAAAAAIBLaC4CAAAAAAAAcAnNRQAAAAAAAAAuobkIAAAAAAAAwCU0FwEAAAAAAAC4hOYiAAAAAAAAAJfQXAQAAAAAAADgEpqLAAAAAAAAAFxCcxEAAAAAAACAS/4PvboP3mx3Yj0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "conduct_experiment(ode_true, ode_trained, 500, \"linear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ2RL0rk8TZC"
      },
      "outputs": [],
      "source": [
        "func = TestODEF(Tensor([[-0.1, -0.5], [0.5, -0.1]]), Tensor([[0.2, 1.], [-1, 0.2]]), Tensor([[-1., 0.]]))\n",
        "ode_true = NeuralODE(func)\n",
        "\n",
        "func = NNODEF(2, 16, time_invariant=True)\n",
        "ode_trained = NeuralODE(func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM3CzrK88TZC"
      },
      "outputs": [],
      "source": [
        "conduct_experiment(ode_true, ode_trained, 3000, \"comp\", plot_freq=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxnp0MRs8TZD"
      },
      "source": [
        "As one can see, Neural ODEs are pretty successful in approximating dynamics. Now let's check if they can be used in a slightly more complicated (MNIST, ha-ha) task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlGAUQWZ8TZD"
      },
      "source": [
        "## Neural ODE inspired by ResNets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrzJNBqe8TZD"
      },
      "source": [
        "In residual networks hidden state changes according to the formula\n",
        "$$\n",
        "h_{t+1} = h_{t} + f(h_{t}, \\theta_{t})\n",
        "$$\n",
        "\n",
        "where $t \\in \\{0...T\\}$ is residual block number and $f$ is a function learned by layers inside the block.\n",
        "\n",
        "If one takes a limit of an infinite number of residual blocks with smaller steps one gets continuous dynamics of hidden units to be an ordinary differential equation just as we had above.\n",
        "\n",
        "$$\n",
        "\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\n",
        "$$\n",
        "\n",
        "Starting from the input layer $h(0)$, one can deï¬ne the output layer $h(T)$ to be the solution to this ODE initial value problem at some time T.\n",
        "\n",
        "Now one can treat $\\theta$ as parameters shared among all infinitesimally small residual blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpOlExGN8TZD"
      },
      "source": [
        "### Testing Neural ODE architecture on MNIST\n",
        "\n",
        "In this section we test the ability of Neural ODE's to be used as a component in more conventional architectures.\n",
        "In particular, we will use Neural ODE in place of residual blocks in MNIST classifier.\n",
        "\n",
        "<img src=\"https://github.com/deburg0/neural-ode/blob/master/assets/mnist_example.png?raw=1\" width=400></img>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7aWLRox8TZD"
      },
      "outputs": [],
      "source": [
        "def norm(dim):\n",
        "    return nn.BatchNorm2d(dim)\n",
        "\n",
        "def conv3x3(in_feats, out_feats, stride=1):\n",
        "    return nn.Conv2d(in_feats, out_feats, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "def add_time(in_tensor, t):\n",
        "    bs, c, w, h = in_tensor.shape\n",
        "    return torch.cat((in_tensor, t.expand(bs, 1, w, h)), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwA2w-Nc8TZD"
      },
      "outputs": [],
      "source": [
        "class ConvODEF(ODEF):\n",
        "    def __init__(self, dim):\n",
        "        super(ConvODEF, self).__init__()\n",
        "        self.conv1 = conv3x3(dim + 1, dim)\n",
        "        self.norm1 = norm(dim)\n",
        "        self.conv2 = conv3x3(dim + 1, dim)\n",
        "        self.norm2 = norm(dim)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        xt = add_time(x, t)\n",
        "        h = self.norm1(torch.relu(self.conv1(xt)))\n",
        "        ht = add_time(h, t)\n",
        "        dxdt = self.norm2(torch.relu(self.conv2(ht)))\n",
        "        return dxdt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emiJHVM08TZE"
      },
      "outputs": [],
      "source": [
        "class ContinuousNeuralMNISTClassifier(nn.Module):\n",
        "    def __init__(self, ode):\n",
        "        super(ContinuousNeuralMNISTClassifier, self).__init__()\n",
        "        self.downsampling = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1),\n",
        "            norm(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),\n",
        "            norm(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),\n",
        "        )\n",
        "        self.feature = ode\n",
        "        self.norm = norm(64)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsampling(x)\n",
        "        x = self.feature(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.avg_pool(x)\n",
        "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
        "        x = x.view(-1, shape)\n",
        "        out = self.fc(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHLce25r8TZE"
      },
      "outputs": [],
      "source": [
        "func = ConvODEF(64)\n",
        "ode = NeuralODE(func)\n",
        "model = ContinuousNeuralMNISTClassifier(ode)\n",
        "if use_cuda:\n",
        "    model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcOrRQdo8TZE"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "img_std = 0.3081\n",
        "img_mean = 0.1307\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(\"data/mnist\", train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                                 torchvision.transforms.ToTensor(),\n",
        "                                 torchvision.transforms.Normalize((img_mean,), (img_std,))\n",
        "                             ])\n",
        "    ),\n",
        "    batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST(\"data/mnist\", train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                                 torchvision.transforms.ToTensor(),\n",
        "                                 torchvision.transforms.Normalize((img_mean,), (img_std,))\n",
        "                             ])\n",
        "    ),\n",
        "    batch_size=128, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT5fqFdY8TZE"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMt8vxpN8TZE"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    num_items = 0\n",
        "    train_losses = []\n",
        "\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(f\"Training Epoch {epoch}...\")\n",
        "    for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        if use_cuda:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses += [loss.item()]\n",
        "        num_items += data.shape[0]\n",
        "    print('Train loss: {:.5f}'.format(np.mean(train_losses)))\n",
        "    return train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo2npo0K8TZE"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    accuracy = 0.0\n",
        "    num_items = 0\n",
        "\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(f\"Testing...\")\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in tqdm(enumerate(test_loader),  total=len(test_loader)):\n",
        "            if use_cuda:\n",
        "                data = data.cuda()\n",
        "                target = target.cuda()\n",
        "            output = model(data)\n",
        "            accuracy += torch.sum(torch.argmax(output, dim=1) == target).item()\n",
        "            num_items += data.shape[0]\n",
        "    accuracy = accuracy * 100 / num_items\n",
        "    print(\"Test Accuracy: {:.3f}%\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "YOUEUHjH8TZF"
      },
      "outputs": [],
      "source": [
        "n_epochs = 5\n",
        "test()\n",
        "train_losses = []\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    train_losses += train(epoch)\n",
        "    test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEe8Ne8R8TZF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "plt.figure(figsize=(9, 5))\n",
        "history = pd.DataFrame({\"loss\": train_losses})\n",
        "history[\"cum_data\"] = history.index * batch_size\n",
        "history[\"smooth_loss\"] = history.loss.ewm(halflife=10).mean()\n",
        "history.plot(x=\"cum_data\", y=\"smooth_loss\", figsize=(12, 5), title=\"train error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVDn5A2a8TZF"
      },
      "source": [
        "```\n",
        "Testing...\n",
        "100% 79/79 [00:01<00:00, 45.69it/s]\n",
        "Test Accuracy: 9.740%\n",
        "\n",
        "Training Epoch 1...\n",
        "100% 1875/1875 [01:15<00:00, 24.69it/s]\n",
        "Train loss: 0.20137\n",
        "Testing...\n",
        "100% 79/79 [00:01<00:00, 46.64it/s]\n",
        "Test Accuracy: 98.680%\n",
        "\n",
        "Training Epoch 2...\n",
        "100% 1875/1875 [01:17<00:00, 24.32it/s]\n",
        "Train loss: 0.05059\n",
        "Testing...\n",
        "100% 79/79 [00:01<00:00, 46.11it/s]\n",
        "Test Accuracy: 97.760%\n",
        "\n",
        "Training Epoch 3...\n",
        "100% 1875/1875 [01:16<00:00, 24.63it/s]\n",
        "Train loss: 0.03808\n",
        "Testing...\n",
        "100% 79/79 [00:01<00:00, 45.65it/s]\n",
        "Test Accuracy: 99.000%\n",
        "\n",
        "Training Epoch 4...\n",
        "100% 1875/1875 [01:17<00:00, 24.28it/s]\n",
        "Train loss: 0.02894\n",
        "Testing...\n",
        "100% 79/79 [00:01<00:00, 45.42it/s]\n",
        "Test Accuracy: 99.130%\n",
        "\n",
        "Training Epoch 5...\n",
        "100% 1875/1875 [01:16<00:00, 24.67it/s]\n",
        "Train loss: 0.02424\n",
        "Testing...\n",
        "100% 79/79 [00:01<00:00, 45.89it/s]\n",
        "Test Accuracy: 99.170%\n",
        "```\n",
        "\n",
        "![train error](https://github.com/deburg0/neural-ode/blob/master/assets/train_error.png?raw=1)\n",
        "\n",
        "After a very rough training procedure of only 5 epochs and 6 minutes of training the model already has test error of less than 1%. Which shows that Neural ODE architecture fits very good as a component in more conventional nets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TRMGlIr8TZF"
      },
      "source": [
        "In their paper, authors also compare this classifier to simple 1-layer MLP, to ResNet with alike architecture, and to same ODE architecture, but in which gradients propagated directly through ODESolve (without adjoint gradient method) (RK-Net).\n",
        "![\"Methods comparison\"](https://github.com/deburg0/neural-ode/blob/master/assets/methods_compare.png?raw=1)\n",
        "<div align=\"center\">Figure from original paper</div>\n",
        "\n",
        "According to them, 1-layer MLP with roughly the same amount of parameters as Neural ODE-Net has much higher test error, ResNet with roughly the same error has much more parameters, and RK-Net with direct backpropagation through ODESolver has slightly higher error and linearly growing memory usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myLW86CF8TZF"
      },
      "source": [
        "In their paper, authors use implicit Runge-Kutta solver with adaptive step size instead of simple Euler's method. They also examine some ODE-Net characteristics.\n",
        "\n",
        "![\"Node attrs\"](https://github.com/deburg0/neural-ode/blob/master/assets/ode_solver_attrs.png?raw=1)\n",
        "\n",
        "<div align=\"center\">ODE-Net characteristics (NFE Forward - number of function evaluations during forward pass)</div>\n",
        "<div align=\"center\">Figure from original paper</div>\n",
        "\n",
        "- (a) Changing tolerable Numerical Error varies the number of steps per forward pass evaluation.\n",
        "- (b) Time spent by the forward call is proportional to the number of function evaluations.\n",
        "- (c) Number of backward evaluations is roughly half the number of forward evaluations, this suggests that adjoint method is more computationally efficient than direct backpropagation through ODESolver.\n",
        "- (d) As ODE-Net becomes more and more trained, it demands more and more evaluations, presumably adapting to the increasing complexity of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc_kLOCF8TZF"
      },
      "source": [
        "## Generative latent function time-series model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoMC9rgX8TZG"
      },
      "source": [
        "Neural ODE seems to be more suitable for continuous sequential data even when this continuous trajectory is in some unknown latent space.\n",
        "\n",
        "In this section we will experiment with generating continuous sequential data using Neural ODE and exploring its latent space a bit.\n",
        "Authors also compare it to the same sequential data but generated with Recurrent Neural Networks.\n",
        "\n",
        "The approach here is slightly different from the corresponding example in authors repository, the one here has a more diverse set of trajectories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGOxZNJm8TZG"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFc6dNoZ8TZG"
      },
      "source": [
        "Training data consists of random spirals, one half of which is clockwise and another is counter-clockwise. Then, random subtimespans of size 100 are sampled from these spirals, having passed through encoder rnn model in reversed order yielding a latent starting state, which then evolves creating a trajectory in the latent space. This latent trajectory is then mapped onto the data space trajectory and compared with the actual data observations. Thus, the model learns to generate data-alike trajectories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-xPlzoQ8TZG"
      },
      "source": [
        "![image.png](https://github.com/deburg0/neural-ode/blob/master/assets/spirals_examples.png?raw=1)\n",
        "<div align=\"center\">Examples of spirals in the dataset</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9DRiHnq8TZG"
      },
      "source": [
        "### VAE as a generative model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiR6REPv8TZG"
      },
      "source": [
        "A generative model through sampling procedure:\n",
        "$$\n",
        "z_{t_0} \\sim \\mathcal{N}(0, I)\n",
        "$$\n",
        "\n",
        "$$\n",
        "z_{t_1}, z_{t_2},...,z_{t_M} = \\text{ODESolve}(z_{t_0}, f, \\theta_f, t_0,...,t_M)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{each } x_{t_i} \\sim p(x \\mid z_{t_i};\\theta_x)\n",
        "$$\n",
        "\n",
        "Which can be trained using variational autoencoder approach:\n",
        "\n",
        "1. Run the RNN encoder through the time series backwards in time to infer the parameters $\\mu_{z_{t_0}}$, $\\sigma_{z_{t_0}}$ of variational posterior and sample from it\n",
        "$$\n",
        "z_{t_0} \\sim q \\left( z_{t_0} \\mid x_{t_0},...,x_{t_M}; t_0,...,t_M; \\theta_q \\right) = \\mathcal{N} \\left(z_{t_0} \\mid \\mu_{z_{t_0}} \\sigma_{z_{t_0}} \\right)\n",
        "$$\n",
        "2. Obtain the latent trajectory\n",
        "$$\n",
        "z_{t_1}, z_{t_2},...,z_{t_N} = \\text{ODESolve}(z_{t_0}, f, \\theta_f, t_0,...,t_N), \\text{ where } \\frac{d z}{d t} = f(z, t; \\theta_f)\n",
        "$$\n",
        "3. Map the latent trajectory onto the data space using another neural network: $\\hat{x_{t_i}}(z_{t_i}, t_i; \\theta_x)$\n",
        "4. Maximize Evidence Lower BOund estimate for sampled trajectory\n",
        "$$\n",
        "\\text{ELBO} \\approx N \\Big( \\sum_{i=0}^{M} \\log p(x_{t_i} \\mid z_{t_i}(z_{t_0}; \\theta_f); \\theta_x) + KL \\left( q( z_{t_0} \\mid x_{t_0},...,x_{t_M}; t_0,...,t_M; \\theta_q) \\parallel \\mathcal{N}(0, I) \\right) \\Big)\n",
        "$$\n",
        "And in case of Gaussian posterior $p(x \\mid z_{t_i};\\theta_x)$ and known noise level $\\sigma_x$\n",
        "$$\n",
        "\\text{ELBO} \\approx -N \\Big( \\sum_{i=1}^{M}\\frac{(x_i - \\hat{x_i} )^2}{\\sigma_x^2} - \\log \\sigma_{z_{t_0}}^2 + \\mu_{z_{t_0}}^2 + \\sigma_{z_{t_0}}^2 \\Big) + C\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbz0Arjx8TZG"
      },
      "source": [
        "Computation graph of the latent ODE model can be depicted like this\n",
        "![vae_model](https://github.com/deburg0/neural-ode/blob/master/assets/vae_model.png?raw=1)\n",
        "<div align=\"center\">Figure from the original paper</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XQRtmyZ8TZG"
      },
      "source": [
        "One can then test how this model extrapolates the trajectory from only its initial moment observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vgd4CXs8TZH"
      },
      "source": [
        "### Defining the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf0eMOTi8TZH"
      },
      "outputs": [],
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(RNNEncoder, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.rnn = nn.GRU(input_dim+1, hidden_dim)\n",
        "        self.hid2lat = nn.Linear(hidden_dim, 2*latent_dim)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Concatenate time to input\n",
        "        t = t.clone()\n",
        "        t[1:] = t[:-1] - t[1:]\n",
        "        t[0] = 0.\n",
        "        xt = torch.cat((x, t), dim=-1)\n",
        "\n",
        "        _, h0 = self.rnn(xt.flip((0,)))  # Reversed\n",
        "        # Compute latent dimension\n",
        "        z0 = self.hid2lat(h0[0])\n",
        "        z0_mean = z0[:, :self.latent_dim]\n",
        "        z0_log_var = z0[:, self.latent_dim:]\n",
        "        return z0_mean, z0_log_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxyuMQja8TZH"
      },
      "outputs": [],
      "source": [
        "class NeuralODEDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, latent_dim):\n",
        "        super(NeuralODEDecoder, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        func = NNODEF(latent_dim, hidden_dim, time_invariant=True)\n",
        "        self.ode = NeuralODE(func)\n",
        "        self.l2h = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.h2o = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, z0, t):\n",
        "        zs = self.ode(z0, t, return_whole_sequence=True)\n",
        "\n",
        "        hs = self.l2h(zs)\n",
        "        xs = self.h2o(hs)\n",
        "        return xs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFjO4_Oe8TZH"
      },
      "outputs": [],
      "source": [
        "class ODEVAE(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, latent_dim):\n",
        "        super(ODEVAE, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = RNNEncoder(output_dim, hidden_dim, latent_dim)\n",
        "        self.decoder = NeuralODEDecoder(output_dim, hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, t, MAP=False):\n",
        "        z_mean, z_log_var = self.encoder(x, t)\n",
        "        if MAP:\n",
        "            z = z_mean\n",
        "        else:\n",
        "            z = z_mean + torch.randn_like(z_mean) * torch.exp(0.5 * z_log_var)\n",
        "        x_p = self.decoder(z, t)\n",
        "        return x_p, z, z_mean, z_log_var\n",
        "\n",
        "    def generate_with_seed(self, seed_x, t):\n",
        "        seed_t_len = seed_x.shape[0]\n",
        "        z_mean, z_log_var = self.encoder(seed_x, t[:seed_t_len])\n",
        "        x_p = self.decoder(z_mean, t)\n",
        "        return x_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNd-V38u8TZH"
      },
      "source": [
        "### Generating dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ20Qy8s8TZH"
      },
      "outputs": [],
      "source": [
        "t_max = 6.29*5\n",
        "n_points = 200\n",
        "noise_std = 0.02\n",
        "\n",
        "num_spirals = 1000\n",
        "\n",
        "index_np = np.arange(0, n_points, 1, dtype=int)\n",
        "index_np = np.hstack([index_np[:, None]])\n",
        "times_np = np.linspace(0, t_max, num=n_points)\n",
        "times_np = np.hstack([times_np[:, None]] * num_spirals)\n",
        "times = torch.from_numpy(times_np[:, :, None]).to(torch.float32)\n",
        "\n",
        "# Generate random spirals parameters\n",
        "normal01 = torch.distributions.Normal(0, 1.0)\n",
        "\n",
        "x0 = Variable(normal01.sample((num_spirals, 2))) * 2.0\n",
        "\n",
        "W11 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05\n",
        "W22 = -0.1 * normal01.sample((num_spirals,)).abs() - 0.05\n",
        "W21 = -1.0 * normal01.sample((num_spirals,)).abs()\n",
        "W12 =  1.0 * normal01.sample((num_spirals,)).abs()\n",
        "\n",
        "xs_list = []\n",
        "for i in range(num_spirals):\n",
        "    if i % 2 == 1: #  Make it counter-clockwise\n",
        "        W21, W12 = W12, W21\n",
        "\n",
        "    func = LinearODEF(Tensor([[W11[i], W12[i]], [W21[i], W22[i]]]))\n",
        "    ode = NeuralODE(func)\n",
        "\n",
        "    xs = ode(x0[i:i+1], times[:, i:i+1], return_whole_sequence=True)\n",
        "    xs_list.append(xs)\n",
        "\n",
        "\n",
        "orig_trajs = torch.cat(xs_list, dim=1).detach()\n",
        "samp_trajs = orig_trajs + torch.randn_like(orig_trajs) * noise_std\n",
        "samp_ts = times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "s3pb01xh8TZI"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 9))\n",
        "axes = axes.flatten()\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.scatter(samp_trajs[:, i, 0], samp_trajs[:, i, 1], c=samp_ts[:, i, 0], cmap=cm.plasma)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbG_tZRZ8TZI"
      },
      "outputs": [],
      "source": [
        "import numpy.random as npr\n",
        "\n",
        "def gen_batch(batch_size, n_sample=100):\n",
        "    n_batches = samp_trajs.shape[1] // batch_size\n",
        "    time_len = samp_trajs.shape[0]\n",
        "    n_sample = min(n_sample, time_len)\n",
        "    for i in range(n_batches):\n",
        "        if n_sample > 0:\n",
        "            t0_idx = npr.multinomial(1, [1. / (time_len - n_sample)] * (time_len - n_sample))\n",
        "            t0_idx = np.argmax(t0_idx)\n",
        "            tM_idx = t0_idx + n_sample\n",
        "        else:\n",
        "            t0_idx = 0\n",
        "            tM_idx = time_len\n",
        "\n",
        "        frm, to = batch_size*i, batch_size*(i+1)\n",
        "        yield samp_trajs[t0_idx:tM_idx, frm:to], samp_ts[t0_idx:tM_idx, frm:to]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a_59Rtx8TZI"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj0EI_fG8TZI"
      },
      "outputs": [],
      "source": [
        "vae = ODEVAE(2, 64, 6)\n",
        "vae = vae.cuda()\n",
        "if use_cuda:\n",
        "    vae = vae.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RghZMEWJ8TZI"
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(vae.parameters(), betas=(0.9, 0.999), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "4Vp2mB5X8TZI"
      },
      "outputs": [],
      "source": [
        "preload = False\n",
        "n_epochs = 20000\n",
        "batch_size = 100\n",
        "\n",
        "plot_traj_idx = 1\n",
        "plot_traj = orig_trajs[:, plot_traj_idx:plot_traj_idx+1]\n",
        "plot_obs = samp_trajs[:, plot_traj_idx:plot_traj_idx+1]\n",
        "plot_ts = samp_ts[:, plot_traj_idx:plot_traj_idx+1]\n",
        "if use_cuda:\n",
        "    plot_traj = plot_traj.cuda()\n",
        "    plot_obs = plot_obs.cuda()\n",
        "    plot_ts = plot_ts.cuda()\n",
        "\n",
        "if preload:\n",
        "    vae.load_state_dict(torch.load(\"models/vae_spirals.sd\"))\n",
        "\n",
        "for epoch_idx in range(n_epochs):\n",
        "    losses = []\n",
        "    train_iter = gen_batch(batch_size)\n",
        "    for x, t in train_iter:\n",
        "        optim.zero_grad()\n",
        "        if use_cuda:\n",
        "            x, t = x.cuda(), t.cuda()\n",
        "\n",
        "        max_len = np.random.choice([30, 50, 100])\n",
        "        permutation = np.random.permutation(t.shape[0])\n",
        "        np.random.shuffle(permutation)\n",
        "        permutation = np.sort(permutation[:max_len])\n",
        "\n",
        "        x, t = x[permutation], t[permutation]\n",
        "\n",
        "        x_p, z, z_mean, z_log_var = vae(x, t)\n",
        "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - torch.exp(z_log_var), -1)\n",
        "        loss = 0.5 * ((x-x_p)**2).sum(-1).sum(0) / noise_std**2 + kl_loss\n",
        "        loss = torch.mean(loss)\n",
        "        loss /= max_len\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch_idx}\")\n",
        "\n",
        "    frm, to, to_seed = 0, 200, 50\n",
        "    seed_trajs = samp_trajs[frm:to_seed]\n",
        "    ts = samp_ts[frm:to]\n",
        "    if use_cuda:\n",
        "        seed_trajs = seed_trajs.cuda()\n",
        "        ts = ts.cuda()\n",
        "\n",
        "    samp_trajs_p = to_np(vae.generate_with_seed(seed_trajs, ts))\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 9))\n",
        "    axes = axes.flatten()\n",
        "    for i, ax in enumerate(axes):\n",
        "        ax.scatter(to_np(seed_trajs[:, i, 0]), to_np(seed_trajs[:, i, 1]), c=to_np(ts[frm:to_seed, i, 0]), cmap=cm.plasma)\n",
        "        ax.plot(to_np(orig_trajs[frm:to, i, 0]), to_np(orig_trajs[frm:to, i, 1]))\n",
        "        ax.plot(samp_trajs_p[:, i, 0], samp_trajs_p[:, i, 1])\n",
        "    plt.show()\n",
        "\n",
        "    print(np.mean(losses), np.median(losses))\n",
        "    clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5gkrW-u8TZJ"
      },
      "outputs": [],
      "source": [
        "spiral_0_idx = 3\n",
        "spiral_1_idx = 6\n",
        "\n",
        "homotopy_p = Tensor(np.linspace(0., 1., 10)[:, None])\n",
        "vae = vae\n",
        "if use_cuda:\n",
        "    homotopy_p = homotopy_p.cuda()\n",
        "    vae = vae.cuda()\n",
        "\n",
        "spiral_0 = orig_trajs[:, spiral_0_idx:spiral_0_idx+1, :]\n",
        "spiral_1 = orig_trajs[:, spiral_1_idx:spiral_1_idx+1, :]\n",
        "ts_0 = samp_ts[:, spiral_0_idx:spiral_0_idx+1, :]\n",
        "ts_1 = samp_ts[:, spiral_1_idx:spiral_1_idx+1, :]\n",
        "if use_cuda:\n",
        "    spiral_0, ts_0 = spiral_0.cuda(), ts_0.cuda()\n",
        "    spiral_1, ts_1 = spiral_1.cuda(), ts_1.cuda()\n",
        "\n",
        "z_cw, _ = vae.encoder(spiral_0, ts_0)\n",
        "z_cc, _ = vae.encoder(spiral_1, ts_1)\n",
        "\n",
        "homotopy_z = z_cw * (1 - homotopy_p) + z_cc * homotopy_p\n",
        "\n",
        "t = torch.from_numpy(np.linspace(0, 6*np.pi, 200))\n",
        "t = t[:, None].expand(200, 10)[:, :, None].cuda()\n",
        "t = t.cuda() if use_cuda else t\n",
        "hom_gen_trajs = vae.decoder(homotopy_z, t)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 5))\n",
        "axes = axes.flatten()\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.plot(to_np(hom_gen_trajs[:, i, 0]), to_np(hom_gen_trajs[:, i, 1]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "z38pm0et8TZJ"
      },
      "source": [
        "torch.save(vae.state_dict(), \"models/vae_spirals.sd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beaGzTBY8TZJ"
      },
      "source": [
        "This is what I got after a night of training\n",
        "![spiral reconstruction with seed](https://github.com/deburg0/neural-ode/blob/master/assets/spirals_reconstructed.png?raw=1)\n",
        "<div align=\"center\">Dots are noisy observations of the original trajectories (blue), <br /> yellow are reconstructed and interpolated trajectories using dots as inputs. <br /> Color of the dots represents time. </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrhQUUjm8TZJ"
      },
      "source": [
        "Reconstuctions of some examples are not very good. Maybe the model is not complex enough or haven't been trained for a long enough time. Anyway, results look very credible.\n",
        "\n",
        "Now lets have a look at what happens if we interpolate the latent variable of the clockwise trajectory to another - the counter-clockwise one.\n",
        "![homotopy](https://github.com/deburg0/neural-ode/blob/master/assets/spirals_homotopy.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhYjGjE78TZJ"
      },
      "source": [
        "Authors also compare reconstructed trajectories using initial moment of time observations of Neural ODE and simple RNN.\n",
        "![ode_rnn_comp](https://github.com/deburg0/neural-ode/blob/master/assets/ode_rnn_comp.png?raw=1)\n",
        "<div align=\"center\">Figure from the original paper</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rd0DO1i8TZJ"
      },
      "source": [
        "## Continuous normalizing flows\n",
        "\n",
        "The original paper also contributes a lot in the topic of Normalizing Flows. Normalizing flows are used when one needs to sample from a complicated distribution originating from a change of variables in some simple distribution (e.q. Gaussian), while still being able to know the probability density of each sample.  \n",
        "They show that using continuous change of variables is much more computationally efficient and interpretable than previous methods._\n",
        "\n",
        "Normalizing flows are very useful in such models as *Variational AutoEncoders*, *Bayesian Neural Networks* and other things in Bayesian setting.\n",
        "\n",
        "This topic, however, is beyond the scope of the present notebook, and those interested are adressed to  the original paper.\n",
        "\n",
        "To tease you a bit:\n",
        "\n",
        "![CNF_NF_comp](https://github.com/deburg0/neural-ode/blob/master/assets/CNF_NF_comp.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq00aPBZ8TZJ"
      },
      "source": [
        "<div align=\"center\">Visualizing the transformation from noise (simple distribution) to data (complicated distribution) for two datasets; <br /> X-axis represents density and samples transformation with \"time\" (for CNF) and \"depth\" (for NF) <br />Figure from the original paper</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an8zaVyh8TZK"
      },
      "source": [
        "This concludes my little investigation of **Neural ODEs**. Hope you found it useful!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp2NoI8J8TZK"
      },
      "source": [
        "# Useful links\n",
        "\n",
        "   - [Original paper](https://arxiv.org/abs/1806.07366)\n",
        "   - [Authors' PyTorch implementation](https://github.com/rtqichen/torchdiffeq)\n",
        "   - [Variational Inference](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)\n",
        "   - [My article on VAE (Russian)](https://habr.com/en/post/331552/)\n",
        "   - [VAE explained](https://www.jeremyjordan.me/variational-autoencoders/)\n",
        "   - [More on Normalizing Flows](http://akosiorek.github.io/ml/2018/04/03/norm_flows.html)\n",
        "   - [Variational Inference with Normalizing Flows Paper](https://arxiv.org/abs/1505.05770)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}